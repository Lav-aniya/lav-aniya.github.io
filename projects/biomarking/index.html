<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>BioMarking | Lav-Aniya&#39;s Site</title>
<meta name="keywords" content="Deep Learning, GANs, Python, Security">
<meta name="description" content="biometric steganography">
<meta name="author" content="Lav-niya">
<link rel="canonical" href="https://lav-aniya.github.io/projects/biomarking/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.1f97f95668440377e5dcde099c302fc9dfe0232ee0eb1ce62fb6c6720e6e67b7.css" integrity="sha256-H5f5VmhEA3fl3N4JnDAvyd/gIy7g6xzmL7bGcg5uZ7c=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://lav-aniya.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://lav-aniya.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://lav-aniya.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://lav-aniya.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://lav-aniya.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://lav-aniya.github.io/projects/biomarking/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" integrity="sha384-wcIxkf4k558AjM3Yz3BBFQUbk/zgIYC2R0QpeeYb+TwlBVMrlgLqwRjRtGZiK7ww" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.js" integrity="sha384-hIoBPJpTUs74ddyc4bFZSM1TVlQDA60VBbJS0oA934VSz82sBx1X7kSx2ATBDIyd" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/auto-render.min.js" integrity="sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
            delimiters: [
                {left: '$$', right: '$$', display: true},
                {left: '$', right: '$', display: false},
                {left: "\\begin{equation}", right: "\\end{equation}", display: true},
                {left: "\\begin{equation*}", right: "\\end{equation*}", display: true},
                {left: "\\begin{align}", right: "\\end{align}", display: true},
                {left: "\\begin{align*}", right: "\\end{align*}", display: true},
                {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
                {left: "\\begin{gather}", right: "\\end{gather}", display: true},
                {left: "\\begin{CD}", right: "\\end{CD}", display: true},
            ],
            throwOnError : false
        });
    });
</script>

<meta property="og:url" content="https://lav-aniya.github.io/projects/biomarking/">
  <meta property="og:site_name" content="Lav-Aniya&#39;s Site">
  <meta property="og:title" content="BioMarking">
  <meta property="og:description" content="biometric steganography">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="projects">
    <meta property="article:tag" content="Deep Learning">
    <meta property="article:tag" content="GANs">
    <meta property="article:tag" content="Python">
    <meta property="article:tag" content="Security">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="BioMarking">
<meta name="twitter:description" content="biometric steganography">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Projects",
      "item": "https://lav-aniya.github.io/projects/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "BioMarking",
      "item": "https://lav-aniya.github.io/projects/biomarking/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "BioMarking",
  "name": "BioMarking",
  "description": "biometric steganography",
  "keywords": [
    "Deep Learning", "GANs", "Python", "Security"
  ],
  "articleBody": "BioMarking is an AI-powered security system that turns a human fingerprint into a “digital safe”. It allows users to hide sensitive text payloads (like passwords, keys, coordinates or anything) directly inside the pixel structure of a fingerprint image.\nUnlike standard steganography tools that treat images as meaningless containers, BioMarking respects the biological constraints of the fingerprint. It embeds data invisibly while ensuring the fingerprint remains biometrically valid. This means the watermarked image can still be used to unlock phone, pass a scanner, or match a database, effectively doubling the utility of the biometric credential.\nThe system operates on a “Zero-knowledge” principle. The payload is encrypted client-side (AES-256) before it ever touches the AI. The Deep Learning model acts purely as a signal carrier, embedding encrypted noise inot biological features without ever understanding the content.\nTech Stack:\nLanguage: Python Frameworks: PyTorch, FastAPI Tools: OpenCV, NumPy Frontend: Next.js The Engineering Challenge The core problem in biometric watermarking is a conflict of interest between two opposing mathematical goals.\nThe Conflict: Steganography requires perturbation. To hide a digital payload (like cryptographic key or ID) inside and image, you must alter the pixel values. To make that data robust against noise or compression, you typically need to alter the image significantly in the frequency domain.\nBiometrics requires precision. Fingerprint recognition algorithms, whether they are classical minutiae extractors or modern Deep Learning matchers (like DeepPrint), rely heavily on high-frequency spatial details. Specifically, they look for ridge bifurcations, endings, and pore locations.\nIf a watermarking algorithm modifies these specific pixels to hide data, it destroys the biometric identity. The scanner sees a “different perseon”, rendering the fingerprint useless for authentication.\nThe Goal: The objective was to build a system that could embed a 32-bit binary payload into a fingerprint image while satisfying three strict constraints:\nBiometric Utility: The watermarked image must match the original identity with a Cosine Similarity score of \u003e0.90. Visual Invisibility: The changes must be statistically indistinguishable from sensor noise (PSNR\u003e30dB). Robust Recovery: the payload must be recoverable with \u003e99% accuracy, even if the image is degraded by noise or blurring. The Approach: Standard steganography techniques (like Least Significant Bit modification) are too fragile; they break under the slightest compression. I utilized a custom Generative Adversarial Network (GAN) to learn anon-linear mapping function: $$ G \\colon ( I_{original}, M_{message} ) \\rightarrow I_{watermarked} $$\nBy training the Generator against a Frozen Biometric Matcher, the network learned to “weave” the binary payload into the null space of the fingerprint, modifying the texture in regions that the human eye ifnores and the biometric scanner doesn’t care about, effectively using the fingerprint ridges as a robust carrier signal.\nData Preprocessing Pipeline (The Input Layer) Fingerprint images are high-resolution data (typically 500x500 pixels or higher) where the defining features, minutiae points, are only a few pixels wide. Fedding raw pixel data directly into a GAN often leads to training instability. To solve this, I made this preprocessing pipeline.\nNormalization: (Scaling to [-1, 1] range)\nBefore any processing occurs, the raw image tensors (which are typically integers in the [0, 255] range or floats [0, 1]) are mathematically re-scaled to the range [-1, 1].\nThe Math: $$ I_{norm} = \\frac{I - 127.5}{127.5} $$ # We chain transformations to map pixel values to the [-1, 1] range. transform = transforms.Compose([ # Convert [0, 255] integer pixels -\u003e [0.0, 1.0] float tensor transforms.ToTensor(), # Normalize: (input - mean) / std # (x - 0.5) / 0.5 maps [0.0, 1.0] -\u003e [-1.0, 1.0] # This aligns the data with the Generator's Tanh activation output transforms.Normalize([0.5], [0.5]) ]) The Reason: The Generator’s output layer uses a Tanh activation function, which bounds outputs strictly between -1 and 1. By matching the input distribution to the output distribution, we prevent “covariate shift”, allowing the gradients to flow smoothly during the early stages of training without exploding or vanishing. Reflective Padding: (Preventing edge artifacts in CNNs)\nOnce normalized, the image must be padded to ensure its dimensions are perfectly divisible by the patch size (128). A common mistake in CNNs is using Zero Padding (black borders), which creates “hard edges” at the image boundaries. Convolutional filters react strongly to these artificial edges.\nTo prevent this, I Implemented Padding.\nThe Logic: We mirror the ridge patterns at the boundary of the image before slicing it.\nThe Result: This maintains the spatial frequency continuity of the fingerprint. The Convolutional layers perceive a continuous pattern, preventing the model from learning edge artifacts that would be easily detectable by steganalysis tools.\n# Calculate dynamic padding # We ensure the image dimensions are perfectly divisible by the stride (128) pad_H = (STRIDE - (H - PATCH_SIZE) % STRIDE) % STRIDE pad_W = (STRIDE - (W - PATCH_SIZE) % STRIDE) % STRIDE # Apply Reflective Padding # \"reflec\" mirros the pixels at the boundary, preventing edge artifacts # that occur with standard Zero-Padding img_padded = F.pad(img_tensor, (0, pad_W, 0, pad_H), \"reflect\") Patching Strategy: (Sliding window technique (128x128 kernels) to handle high-res images)\nInstead of processing the full fingerprint at once, the system slices the tensor $ I \\in \\mathbb{R}^{1 \\times H \\times W} $ into overlapping patches using PyTorch’s unfold operation.\nKernel Size: 128 x 128 pixels. Stride: 128 pixels (during training) or overlapping with weighted averaging (during inference). This transformation converts our dataset from $ N $ large, global images into $ N \\times K $ smaller, focused texture patches. This forces the Generator to learn local ridge characteristics (micro-features) rather than trying to memorize global finger shapes. This makes the watermark robust across different sensor types and resolutions.\n# Sliding Window Extraction (Unfold) # \"unfold\" extracts sliding local blocks from a large image tensor # Input: [1, Channels, Height, Width] -\u003e Output: [1, Channels*128*128, Num_Patches] patches = F.unfold(img_padded, kernel_size=128, stride=128) # Reshape into Batch Dimension # We transpose and reshape so PyTorch treats every patch as an independent sample # New Shape: [Num_Patches, Channels, 128, 128] patches = patches.permute(0, 2, 1).contiguous().view(-1, C, 128, 128) Latent Feature Fusion: (How a 32-bit text string is expanded to match 2D feature maps)\nA key architectural challenge was feeding a 1D text string (the payload) into a 2D Convolutional Network. A naive approach, such as using a Dense layer to flatten the image, destroys spatial correspondence. Instead, I used Latent Feature Fusion:\nProjection: The 32-bit binary message vector is projected to a higher dimension (e.g., 128 channels) using a fully connected layer.\nBroadcasting: This vector is then “stretched” (repeated) across the spatial dimensions (16 x 16) to match the feature map size at the Generator’s bottleneck.\nBy boradcasting the message, every pixel in the latent space “knows” the secret message. This provides masssive redundancy. It allows the Generator to hide bits locally in every region of the fingerprint. Consequently, even if the final image is cropped or partially obscured, the message can often still be recovered from the remaining fragments.\n# Define a prjectino layer to expand the 32-bit message self.msg_proc = nn.Sequential( nn.Linear(MESSAGE_LENGTH, 16*16*16), nn.ReLU() ) # Execution # Project and reshape the 1D message into a 3D feature volume # InputL [Batch, 32] -\u003e Ouput: [Batch, 16, 16, 16] m = self.msg_proc(msg).view(-1, 16, 16, 16) # Fuse (Concatenate) witht the image bottleneck features # IMage Features (128 ch) + Message volume (16 ch) = 144 Channels x = torch.cat([e3, m], dim=1) The Generator Architecture The Generator handles the writing process. Its job is to take the normalized fingerprint and the message tensor, then output a watermarked image that looks identical to the original.\nI chose a U-Net architecture for this. While ResNets are standard for classification, they often fail at pixel-perfect reconstruction. U-Nets were originally desgined for biomedical segmentaion, which makes them ideal for preserving the fine, high-frequency structures found in fingerprints.\nWhy Skip Connections Matter: (Skip Connections (Why they are essential for ridge preservation).)\nIn a standard Encoder-Decoder, the image is compressed down to a tiny bottleneck and then reconstructed. During this downsampling, fine spatial details, specifically minutiae points (ridge endings and bifurications) are often lost.\nIf the Generator blurs a ridge ending even slightly, the biometric scanner will reject the user.\nTo fix this, I used SKip Connections.\nMechanism: These are direct links connecting the Encoder layers to the corresponding Decoder layers. Result: The network passes high-resolution spatial information (the ridge structure) from the input directly to the output, bypassing the bottleneck entirely. Benefit: The bottleneck doesn’t have to memorize the entire image. It only needs to calculate the residual noise (the “delta”) required to embed the watermark. # The U-Net structure allows us to pass spatial details across the network d1 = self.dec1(x) d1_skip = torch.cat([d1, e2], dim=1) # Skip connection from Encoder 2 d2 = self.dec2(d1_skip) d2_skip = torch.cat([d2, e1], dim=1) # Skip connection from Encdoder 1 # The final layer generates the \"noise\" (residual) to be added residual = self.dec3(d2_skip) # The final reuslt is the original image + the generated residual return torch.clamp(img + residual, -1.0, 1.0) Bottleneck: Feature Fusion (Where the Message Tensor is fused with Image Features)\nThe center of the U-Net is where the visual data meets the text data.\nConcatenation: The Encoder produces a feature map of shape ($ 128 \\times 16 \\times 16 $). The spatially replicated message tensor (also $ 16 \\times 16 $) is concatenated along the channel dimension.\nResidual Processing: This combined tensor passes through a series of Residual Blocks (ResBlocks).\nReasoning: Stacking standard convolution layers here causes signal degradation. ResBLocks allow the network to make complex decisions about where to place the bits. The model learns to hide data in complex texture regions (where ridges intersect) rather than in smooth valleys, making the watermark harder to detect visually. class ResBlock(nn.Module): \"\"\" Residual Block used in the bottleneck Allows the model to learn complex mappings without vanishing gradient \"\"\" def __init__(self, channels): super(ResBlock, self).__init__() self.block = nn.Sequential( nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1, bias=False), nn.BatchNorm2d(channels), nn.ReLU(inplace=True), nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1, bias=False), nn.BatchNorm2d(channels) ) def forward(self, x): return x + self.block(x) # The residual connection Output Layer: (Tanh activation for pixel generation)\nThe Decoder upsamples the features back to the original resolution ($ 128 \\times 128 $) using Transposed Convolution. The final layer maps the 64 feature channels down to 1 channel (Grayscale) and applies a Tanh activation function.\nUpsampling: Unlike standard interpolation, Transposed Convolutions utilize learnable weights to “paint” the pixels back in, ensuring the generated ridge details are sharp. Consraint: Tanh bounds the pixel output strictly to [-1, 1] Consistency: This matches the input normalization from the preprocessing step, ensuring the Generator produces valid pixel values without needing to clip the data, which would destroy information. # The Decoder layers (Upsampling + Channel Reduction) # Input from Bottleneck: 144 channels (16x16) # 1. Upsample 16x16 -\u003e 32x32 # Input: 144 channels. Output: 64 channels. self.dec1 = nn.Sequential( nn.ConvTranspose2d(144, 64, kernel_size=4, stride=2, padding=1), nn.ReLU() ) # 2. Upsample 32x32 -\u003e 64x64 # Input: 128 channels (64 from dec1 + 64 from skip connection). Output: 32. self.dec2 = nn.Sequential( nn.ConvTranspose2d(128, 32, kernel_size=4, stride=2, padding=1), nn.ReLU() ) # 3. Output Layer: Upsample 64x64 -\u003e 128x128 # Input: 64 channels (32 from dec2 + 32 from skip connection). Output: 1. self.dec3 = nn.Sequential( nn.ConvTranspose2d(64, 1, kernel_size=4, stride=2, padding=1), nn.Tanh() # Final activation: [-1, 1] range ) The EXtractor Architecture The Extractor acts as the “Reader”. Its sole purpose is to examine a watermarked fingerprint patch ($ 128 \\times 128 $) and recover the original 32-bit binary payload.\nWhile the Generator is a complex U-Net, the Extractor is a focused Convolutional Neural Network (CNN) designed for multi-label classification.\nBackbone: Hierarchial Feature Extraction (Standard CNN Classifier)\nThe network follows a standard deep CNN architecture to progessively reduce spatial dimensions while increasing feature depth.\nlayers: A stacj if 4 Convolutional Blocks. Components: Each block consists of Conv2d $\\rightarrow$ BatchNorm2d $\\rightarrow$ LeakyRelU. Function: The early layers detect low-level texture variations (edges, ridges), while deeper layers aggregate these into high-level abstract features representing the hidden signal. The final spatial map is flattened into a dense vector. class Extractor(nn.Module): def __init__(self): super().__init__() # Standard CNN Architecture # Progressively downsamples 128x128 image to 8x8 feature map self.main = nn.Sequential( nn.Conv2d(1, 32, 4, 2, 1), nn.LeakyReLU(0.2), nn.Conv2d(32, 64, 4, 2, 1), nn.BatchNorm2d(64), nn.LeakyReLU(0.2), nn.Conv2d(64, 128, 4, 2, 1), nn.BatchNorm2d(128), nn.LeakyReLU(0.2), nn.Conv2d(128, 256, 4, 2, 1), nn.BatchNorm2d(256), nn.LeakyReLU(0.2), nn.Flatten() ) The Noise Layer: (Differentiable augmentations (Gaussian noise/blur) injected during training to force robust encoding) The defining feature of the Extractor is how it is trained. In a naive steganogaraphy setup, if you feed clean data from the Generator directly to the Extractot, the Generator learns to hide data in the Least Singificant Bits (LSB) of the pixels.\nProblem: LSB watermarking is invisible but fragile. It breaks instantly if the image is scanned, compressed (JPEG), or blurred.\nSolution: I implemented a Differentiable Distortion Layer inside the training loop. $$ I_{input} = G(I) + N(0, \\sigma_{noise}) $$\nResult: During training, the Extractor never sees a “clean” watermark. It is forced to read the message through a storm of Gaussian noise. Because the noise is differentiable, gradients flow thorugh it. The Extractor effectively tells the Generator via backpropagation: “I can’t read this signal when it’s noisy; write it louder and deeper”.\nThis forces the GEnerator to abandon fragile LSB tactics and instead encode the watermark into robust, low-frequency spatial features that survive real-worl degradation.\n# Generate Fake Image fake_img = generator(real_img, msg) # Differentiable Augmentation # We inject random noise BEFORE the Extractor sees the image # This forces the Generator to learn robust features that survive corruption noise = (torch.rand_like(fake_img) - 0.5) * 0.4 # High intensity noise fake_img_noisy = (fake_img + noise).clamp(-1.0, 1.0) # Extract from Noisy Image logits = extractor(fake_img_noisy) The Output head:\nThe flattened feature vector passes through Multi-layer Perceptron (MLP) ending in a final Linear layer with 32 output neurons.\nActivation: Unlike a multiclass classifier (which used Softmax), we use Sigmoid. Reasoning: This treats the problem as Multi-Label Classification. Each of the 32 bits is an independent Bernoulli variable (0 or 1). The network outputs a probability for each bit position independently, ensuring high accuracy even if one of the string is harder to decode than another. # The classification Head # Input: Flattened features (256 * 8 * 8) # Output: 32 independent probabilities self.head = nn.Sequential( nn.Linear(256*8*8, 1024), nn.ReLU(), nn.Linear(1024, 256), nn.ReLU(), nn.Linear(256, 32) # The 32-bit(MESSAGE_LENGTH) payload ) # Note: We use BCEWithLogitsLoss during training, # which applies Sigmoid activation numerically stably. The Critic Architecture The Critc serves as the “Judge”. Its objective is to distinguish between original fingerprints and watermarked fingerprints. Unlike a standard GAN Discriminator which classifies inputs as “Real” or “Fake” (0 or 1), the Critic estimates the Earth Mover’s Distance (Wasserstein Distance) between the data distributions.\nI implemented this using the Wasserstein GAN with Gradient Penalty (WGAN-GP) framework to prevent the mode collapse and vanishing gradient problems common in standard GANs.\nThe Architecture: (Wasserstein GAN with Gradient Penalty (WGAN-GP))\nThe Critic is a strided Convolutional Neural Network designed to aggressively downsample spatial information into a scalar score.\nLayers: 3 blocks of Conv2d (kernel 4, stride 2) $\\rightarrow$ LeakyReLU. Design Choice: No Batch Normalization. WGAN-GP penalizes the norm of the gradient with respect to each input independently. Batch Normalization creates correlations between samples in a batch, which invalidates the theoretical guarantees of the gradient Penalty. Output: A single Linear layer with no acitvation function. The output is an unbounded scalar value representing the “realness” score, not a probability. class Critic(nn.Module): def __init__(self): super().__init__() # Strided CNN to Downsample 128x128 -\u003e Scalar Score # NOte: No BatchNorm is used, as required by WGAN-GP self.main = nn.Sequential( nn.Conv2d(1, 32, 4, 2, 1), nn.LeakyReLU(0.2), nn.Conv2d(32, 64, 4, 2, 1), nn.LeakyReLU(0.2), nn.Conv2d(64, 128, 4, 2, 1), nn.LeakyReLU(0.2), nn.Flatten(), nn.Linear(128 * 16 * 16, 1) # Outputs raw score (unbounded) ) def forward(self, x): return self.main(x) The Wasserstein Objective:\nStandard GANs use Binary Cross Entropy (BCE) loss, which minimizes the Jensen-Shannon divergence. This metric fails when the distributions of Real and Fake images do not overlap significantly, causing gradients to vanish and training to stall.\nThe Wasserstein metric provides a menaningful gradient everywhere, even when the generated images are poor quality.\nMechanism: The Critic tries to maximize the difference between scores for real images $D(x)$ and fake images $D(G(z))$. Result: This creates a linear gradient landscape, allowing the Generator to learn continuously without plateauing. Gradient Penalty (1-Lipschitz Constraint):\nFor the Wasserstein mathematical theory to hold, the Critic must satisfy the 1-Lipschitz constraint (the rate of change of the function cannot exceed 1).\nOld Method: Weight CLipping (forcing weights to be between -0.01 and 0.01). This often leads to exploding or vanishing gradients. My Implementation: Gradient Penalty. I sample random points along the straight ines connecting real and fake images. I then penalize the model if the gradient norm at these points deviates from 1. $$ L_{GP} = \\mathbb{E} [( \\parallel \\nabla_{\\hat{x}}D(\\hat{x}) \\parallel_{2} - 1 )^{2}] $$ Outcome: This ensures stable training dynamics and forces the Generator to match the high-frequency texture statistics of the real fingerprints, rather than just their general shape. def gradient_penalty(critic, real, fake, device=DEVICE): # 1. Random Weighting (Interpolation) # Sample random points along the line connecting Real and Fake images alpha = torch.rand(real.size(0), 1, 1, 1, device=device) interpolates = (alpha * real + (1 - alpha) * fake).requires_grad_(True) # 2. Calculate Critic Scores d_interpolates = critic(interpolates) # 3. Calculate Gradients w.r.t. Inputs grads = torch.autograd.grad( outputs=d_interpolates, inputs=interpolates, grad_outputs=torch.ones_like(d_interpolates), create_graph=True, retain_graph=True, only_inputs=True )[0] # 4. Enforce 1-Lipschitz Constraint # Penalize if gradient norm moves away from 1.0 grads = grads.view(grads.size(0), -1) gp = ((grads.norm(2, dim=1) - 1) ** 2).mean() return gp The Biometric Matcher The Biometric Matcher serves as the “Security Guard” of the architecture. Its sole responsibility is ot enforce the constraint that the watermarked fingerprint ($I_{wm}$) must be accepted by authentication systems as the same identity as the original fingerprint ($I_{orig}$).\nTo achieve this, I utilized DeepPrint, a state-of-the-art fixed-length fingerprint extractor based on the Inception-ResNet-v2 backbone.\nThe Architecture: DeepPrint\nUnlike traditional minutiae extractors (which output a vairable list of points), DeepPrint maps a fingerprint image to a fixed-size Identity Vector (embedding) in a high-dimensional hyperspace (typically $\\mathbb{R}^{192}$ or $\\mathbb{R}^{256}$).\nInput: $299 \\times 299$ Grayscale image. Output: A dense feature vector representing the unique ridge topology. Training State:Frozen. The weights of this model are pre-trained on millions of identities and are not updated during our training loop. We treat the Matcher as a differentiable “black box” judge. class Matcher(nn.Module): def __init__(self, weights_path=None): super().__init__() # Load the Inception_ResNet backbone self.deepprint_model = DeepPrint_Tex(num_fingerprints=8000, texture_embedding_dims=256) # FREEZE WEIGHTS # We do not train the matcher; we only use it to calculate loss self.deepprint_model.eval() for param in self.deepprint_model.parameters(): param.requires_grad = False The Mechanism: Cosine Similarity Loss\nTo quantify “Identity Preservation”, we cannot simply compare pixel values (L2 loss), as two images can have different pixels but the same identity (e.g., different lightning). Instead, we compare their feature vectors.\nI implemented the Cosine Embedding Loss to maximize the similarity between the Original and Watermarked vectors.\nForward Pass: $$ v_{orig} = M(I_{orig}) $$ $$ v_{wm} = M(G(I_{orig}, msg)) $$\nHowever, in production, the DeepPrint library has a known bug where it crashes when processing a single image (Batch Size = 1). Since users upload one file at a time, I had to workaround in the forward method to duplicate the tensor on the fly:\ndef forward(self, x): # REsize to 299x299 (DeepPrint requirement) x_resized = F.interpolate(x, size=(299, 299), mode='bilinear', align_corners=False) # FIx: # DeepPrint library crashes on single images (Batch=1) # We duplicate the tensor to fake a batch size of 2 to allow inference if x_resized.shape[0] == 1: x_batch = torch.cat([x_resized, x_resized], dim=0) output = self.deepprint_model(x_batch) return output.texture_embeddings[0].unsqueeze(0) return self.deepprint_model(x_resized).texture_embeddings Loss Calculation: We calculate the cosine of the angle between the two vectors. $$ Similarity = \\frac{v_{orig} . v_{wm}}{\\parallel v_{orig} \\parallel \\parallel v_{wm} \\parallel} $$\nThe loss function minimizes the distance (maximizing similarity): $$ L_{bio} = 1 - CosineSimilarity(v_{orig}, v_{wm}) $$\ndef calculate_biometric_score(matcher, img_orig, img_wm): with torch.no_grad(): # Fet Identity Vectors (Embeddings) vec_orig = matcher(img_orig) vec_wm = matcher(img_wm) # Calculate COsine Similarity # Score range: [-1, 1]. we aim for \u003e 0.90 score = F.cosine_similarity(vec_orig, vec_wm, dim=1).mean().item() return score Why Cosine Similarity? In high-dimensional deep learning spaces, the magnitude of the vector often represents image quality/contrast, while the direction(angle) represents the semantic identity. By forcing the angles to align, we ensure the identity remains constant regardless of local pixel perturbations.\nGradient Flow: The “Feature Inversion\nThe most critical concept here is Backpropagation through a Frozen Network.\nEven though the Matcher’s weights as static (requires_grad=False), the network remains part of the computational graph.\nWhen $L_{bio}$ is calculated, the error signal is generated at the output of the Matcher. This gradient flows backwards through the layers of the Inception-ResNet. It reaches the input pixels of the watermarked image ($I_{wm}$). Since $I_{wm}$ was created by the Generator, the gradients continue flowing into the Generator’s weights. Result: The Generator receives precise, high-level feedback. It is essentially told: “You modified a ridge at (x,y) that pushed the Identity Vector 5 degrees off-target. Fix it.” This forces the Generator to perform a learned Feature Inversion, reconstructing pixels that satisfy the biometric constraints.\nThe “Null Space” Hypothesis\nBy simultaneously minimizing BIometric Loss ($L_{bio}$) and maximizing Data Extraction ($L_{extract}$), the Generator implicitly learns to identify the Null Space of the fingerprint features.\nNull Space: Regions or frequency nands in the image that do not contribute to the identity vector (e.g., texture between ridges, minor pore shape variations).\nOutcome: The Generator learns to hide the payload specifically in these “safe zones”, leaving the critical minutiae points (used for matching) statistically intact.\nThe Phases Training Curriculum Training a multi-network system end-to-end from scratch often leads to failure. If the Generator produces garbage output, the Extractor cannot learn to read. If the Extractor outputs random guesses, the Generator receives no meaningful gradient signal to improve.\nTo resolve this dependency loop, I implemented a four-stage curriculum that progressively increases task complexity.\nPhase 1: Extractor Warmup\nObjective: Solve the “Cold Start” problem for the Reader. Before the Generator is even initialized, the Extractor must learn to detect binary patterns in image textures.\nMechanism: I trained the Extractor (E) in isolation using Synthetic Data. Instead of waiting for the GEnerator, I mathematically injected the watermark signal into real fingerprints using a simple additive formula: $$ I_{synthetic} = I_{real} + (\\alpha \\times M_{pattern}) + N(0, \\sigma) $$\nLogic: By creating a rudimentry, high-amplitude systhetic watermark and corrupting it with noise, we force the Extractor to learn the fundamental frequency of the payload.\nOutcome: The Extractor achieves \u003e90% bit accuracy on synthetic data, ensuring that when the Generator starts training, it receives meaningful gradients immediately.\nPhase 2: Generator Mimic\nObjective: Teach the Writer how to draw a fingerprint. If the Generator tries to hide data before it knows how to generate a valid fingerprint, it will collapse into noise.\nMechanism: The Generator (G) is trained as a pure Autoencoder. The adversarial and extraction losses are disabled. $$ L_{mimic} = \\lambda_{L1} \\parallel G(I) - I\\parallel_{1} + \\lambda_{VGG}L_{perceptual} $$\nLogic: THis forces the Generator to learn the Identity Mapping function. It learns to compress and reconstruct ridge bifuricatoins and pore details pixel-perfectly.\nOutcome: The Generator produces clean fingerprint images (PSNR \u003e 35dB) that look identical to input but contain no hidden data yet.\nPhase 3: Joint Training\nObjective: Connect the Writer and Reader. We unlock both networks and connect them. At this stage, there is no Critic (Discriminator), so the training is purely cooperative.\nMechanism: The Generator minimizes reconstruction loss and extraction loss. The Extractor maximizes bit accuracy.\nLogic: Since $E$ is already warmed up (Phase 1), it guides $G$ on how to modify pixels to encode data. $G$ learns to inject the signal, and $E$ fine-tunes itself to read $G’s$ specific encoding style.\nOutcome: The system works, data is recoverable, but the watermark is often visibile. Without a critic to penalize it, the Generator takes the path of least resistance, creating visible high-frequency artifacts (like checkerboard patterns) because they are easiest for the Extractor to read.\nPhase 4: Adversarial \u0026 Biometric Tuning\nObjective: Enforce Invisibility and Identity. This is the final production phase. We introduce the Critic $(C)$ and the Biometric Matcher ($M$). The Generator is now subjected to a constrained multi-objective optimization problem, fighting a war on three fronts:\nRobustness: Satisfy the Extractor (keep bit accuracy \u003e 99%) Invisibility: Fool the Critic (minimize Wasserstein distance to the real data distribution) Utility: Satisfy the Matcher (maintain Cosine Similarity \u003e 0.90) The final loss function becomes a weighted equilibrium: $$ L_{total} = \\lambda_{adv}L_{WGAN} + \\lambda_{bio}L_{Cosine} + \\lambda_{img}L_{L1} $$\nThis curriculum prevents Mode Collapse and ensures the model settles into a local minimum where the watermark is invisible to the human eye, statistically similar to sensor noise, yet perfectly readable by the algorithm.\n# The Generator must be balance 4 competing goals simultaneously # Extraction Loss # Can the Extractor recover the 32-bit payload? e_loss = extractor_loss_fn(logits_g, msg_smooth) # Adversarial Loss (invisibility) # Does the image look real to the Critic? adv_loss = -critic(fake_g).mean() # Biometric Loss (indentity) # Does the identity vector match the original? (DeepPrint) match_loss = F.l1_loss(matcher(fake_g), matcher(real_img)) # Perceptual Loss (Quality) # Do the texture statistics match VGG features? p_loss = perceptual_loss(real_img, fake_g) # FInal equilibrium # we weigh the losses to prioritize Data Recovery (300.0) and Biometric (75.0) g_loss = (e_loss * 300.0) + (adv_loss * 1.0) + (match_loss * 75.0) + (p_loss * 200.0) Results \u0026 Metrics Visual: (PSNR/SSIM score) Biometric: (Cosine Similarity score (\u003e0.90 threshold)) Recovery: (Bit Accuracy rate under attack) ",
  "wordCount" : "4261",
  "inLanguage": "en",
  "datePublished": "0001-01-01T00:00:00Z",
  "dateModified": "0001-01-01T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Lav-niya"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://lav-aniya.github.io/projects/biomarking/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Lav-Aniya's Site",
    "logo": {
      "@type": "ImageObject",
      "url": "https://lav-aniya.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://lav-aniya.github.io/" accesskey="h" title="Lav-Aniya&#39;s Site (Alt + H)">Lav-Aniya&#39;s Site</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://github.com/Lav-aniya" title="&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-width=&#34;2&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34;&gt;&lt;path d=&#34;M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22&#34;&gt;&lt;/path&gt;&lt;/svg&gt;">
                    <span><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"></path></svg></span>
                    
                </a>
            </li>
            <li>
                <a href="https://www.linkedin.com/in/your-profile-name/" title="&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-width=&#34;2&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34;&gt;&lt;path d=&#34;M16 8a6 6 0 0 1 6 6v7h-4v-7a2 2 0 0 0-2-2 2 2 0 0 0-2 2v7h-4v-7a6 6 0 0 1 6-6z&#34;&gt;&lt;/path&gt;&lt;rect x=&#34;2&#34; y=&#34;9&#34; width=&#34;4&#34; height=&#34;12&#34;&gt;&lt;/rect&gt;&lt;circle cx=&#34;4&#34; cy=&#34;4&#34; r=&#34;2&#34;&gt;&lt;/circle&gt;&lt;/svg&gt;">
                    <span><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M16 8a6 6 0 0 1 6 6v7h-4v-7a2 2 0 0 0-2-2 2 2 0 0 0-2 2v7h-4v-7a6 6 0 0 1 6-6z"></path><rect x="2" y="9" width="4" height="12"></rect><circle cx="4" cy="4" r="2"></circle></svg></span>
                    
                </a>
            </li>
            <li>
                <a href="mailto:lavaniya.nen@gmail.com" title="&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-width=&#34;2&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34;&gt;&lt;path d=&#34;M4 4h16c1.1 0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1 0-2-.9-2-2V6c0-1.1.9-2 2-2z&#34;&gt;&lt;/path&gt;&lt;polyline points=&#34;22,6 12,13 2,6&#34;&gt;&lt;/polyline&gt;&lt;/svg&gt;">
                    <span><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M4 4h16c1.1 0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1 0-2-.9-2-2V6c0-1.1.9-2 2-2z"></path><polyline points="22,6 12,13 2,6"></polyline></svg></span>
                    
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      BioMarking
    </h1>
    <div class="post-description">
      biometric steganography
    </div>
    <div class="post-meta">21 min&nbsp;·&nbsp;Lav-niya

</div>
  </header> 
  <div class="post-content"><p>BioMarking is an AI-powered security system that turns a human fingerprint into a &ldquo;digital safe&rdquo;. It allows users to hide sensitive text payloads (like passwords, keys, coordinates or anything) directly inside the pixel structure of a fingerprint image.</p>
<p>Unlike standard steganography tools that treat images as meaningless containers, BioMarking respects the biological constraints of the fingerprint. It embeds data invisibly while ensuring the fingerprint <strong>remains biometrically valid</strong>. This means the watermarked image can still be used to unlock phone, pass a scanner, or match a database, effectively doubling the utility of the biometric credential.</p>
<p>The system operates on a &ldquo;Zero-knowledge&rdquo; principle. The payload is encrypted client-side (AES-256) before it ever touches the AI. The Deep Learning model acts purely as a signal carrier, embedding encrypted noise inot biological features without ever understanding the content.</p>
<p><strong>Tech Stack:</strong></p>
<ul>
<li><strong>Language:</strong> Python</li>
<li><strong>Frameworks:</strong> PyTorch, FastAPI</li>
<li><strong>Tools:</strong> OpenCV, NumPy</li>
<li><strong>Frontend:</strong> Next.js</li>
</ul>
<h2 id="the-engineering-challenge">The Engineering Challenge<a hidden class="anchor" aria-hidden="true" href="#the-engineering-challenge">#</a></h2>
<p>The core problem in biometric watermarking is a conflict of interest between two opposing mathematical goals.</p>
<ul>
<li><strong>The Conflict:</strong>
<ul>
<li>
<p><strong>Steganography requires perturbation</strong>. To hide a digital payload (like cryptographic key or ID) inside and image, you must alter the pixel values. To make that data robust against noise or compression, you typically need to alter the image significantly in the frequency domain.</p>
</li>
<li>
<p><strong>Biometrics requires precision.</strong> Fingerprint recognition algorithms, whether they are classical minutiae extractors or modern Deep Learning matchers (like DeepPrint), rely heavily on high-frequency spatial details. Specifically, they look for <strong>ridge bifurcations, endings, and pore locations.</strong></p>
</li>
</ul>
</li>
</ul>
<p>If a watermarking algorithm modifies these specific pixels to hide data, it destroys the biometric identity. The scanner sees a &ldquo;different perseon&rdquo;, rendering the fingerprint useless for authentication.</p>
<ul>
<li><strong>The Goal:</strong></li>
</ul>
<p>The objective was to build a system that could embed a <strong>32-bit binary payload</strong> into a fingerprint image while satisfying three strict constraints:</p>
<ol>
<li><strong>Biometric Utility:</strong> The watermarked image must match the original identity with a Cosine Similarity score of &gt;0.90.</li>
<li><strong>Visual Invisibility:</strong> The changes must be statistically indistinguishable from sensor noise (PSNR&gt;30dB).</li>
<li><strong>Robust Recovery:</strong> the payload must be recoverable with &gt;99% accuracy, even if the image is degraded by noise or blurring.</li>
</ol>
<ul>
<li><strong>The Approach:</strong>
Standard steganography techniques (like Least Significant Bit modification) are too fragile; they break under the slightest compression.</li>
</ul>
<p>I utilized a custom <strong>Generative Adversarial Network (GAN)</strong> to learn anon-linear mapping function:
$$ G \colon ( I_{original}, M_{message} ) \rightarrow I_{watermarked} $$</p>
<p>By training the Generator against a <strong>Frozen Biometric Matcher</strong>, the network learned to &ldquo;weave&rdquo; the binary payload into the <strong>null space</strong> of the fingerprint, modifying the texture in regions that the human eye ifnores and the biometric scanner doesn&rsquo;t care about, effectively using the fingerprint ridges as a robust carrier signal.</p>
<h2 id="data-preprocessing-pipeline-the-input-layer">Data Preprocessing Pipeline (The Input Layer)<a hidden class="anchor" aria-hidden="true" href="#data-preprocessing-pipeline-the-input-layer">#</a></h2>
<p>Fingerprint images are high-resolution data (typically 500x500 pixels or higher) where the defining features, minutiae points, are only a few pixels wide. Fedding raw pixel data directly into a GAN often leads to training instability. To solve this, I made this preprocessing pipeline.</p>
<ul>
<li>
<p><strong>Normalization:</strong> (Scaling to [-1, 1] range)</p>
<p>Before any processing occurs, the raw image tensors (which are typically integers in the [0, 255] range or floats [0, 1]) are mathematically re-scaled to the range [-1, 1].</p>
<ul>
<li><strong>The Math:</strong> $$ I_{norm} = \frac{I - 127.5}{127.5} $$</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># We chain transformations to map pixel values to the [-1, 1] range.</span>
</span></span><span style="display:flex;"><span>transform <span style="color:#f92672">=</span> transforms<span style="color:#f92672">.</span>Compose([
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Convert [0, 255] integer pixels -&gt; [0.0, 1.0] float tensor</span>
</span></span><span style="display:flex;"><span>    transforms<span style="color:#f92672">.</span>ToTensor(),
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Normalize: (input - mean) / std</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># (x - 0.5) / 0.5 maps [0.0, 1.0] -&gt; [-1.0, 1.0]</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># This aligns the data with the Generator&#39;s Tanh activation output</span>
</span></span><span style="display:flex;"><span>    transforms<span style="color:#f92672">.</span>Normalize([<span style="color:#ae81ff">0.5</span>], [<span style="color:#ae81ff">0.5</span>])
</span></span><span style="display:flex;"><span>])
</span></span></code></pre></div><ul>
<li><strong>The Reason:</strong> The Generator&rsquo;s output layer uses a <code>Tanh</code> activation function, which bounds outputs strictly between -1 and 1. By matching the input distribution to the output distribution, we prevent &ldquo;covariate shift&rdquo;, allowing the gradients to flow smoothly during the early stages of training without exploding or vanishing.</li>
</ul>
</li>
<li>
<p><strong>Reflective Padding:</strong> (Preventing edge artifacts in CNNs)</p>
<p>Once normalized, the image must be padded to ensure its dimensions are perfectly divisible by the patch size (128). A common mistake in CNNs is using Zero Padding (black borders), which creates &ldquo;hard edges&rdquo; at the image boundaries. Convolutional filters react strongly to these artificial edges.</p>
<p>To prevent this, I <strong>Implemented Padding</strong>.</p>
<ul>
<li>
<p><strong>The Logic:</strong> We mirror the ridge patterns at the boundary of the image before slicing it.</p>
</li>
<li>
<p><strong>The Result:</strong> This maintains the <strong>spatial frequency continuity</strong> of the fingerprint. The Convolutional layers perceive a continuous pattern, preventing the model from learning edge artifacts that would be easily detectable by steganalysis tools.</p>
</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Calculate dynamic padding</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># We ensure the image dimensions are perfectly divisible by the stride (128)</span>
</span></span><span style="display:flex;"><span>pad_H <span style="color:#f92672">=</span> (STRIDE <span style="color:#f92672">-</span> (H <span style="color:#f92672">-</span> PATCH_SIZE) <span style="color:#f92672">%</span> STRIDE) <span style="color:#f92672">%</span> STRIDE
</span></span><span style="display:flex;"><span>pad_W <span style="color:#f92672">=</span> (STRIDE <span style="color:#f92672">-</span> (W <span style="color:#f92672">-</span> PATCH_SIZE) <span style="color:#f92672">%</span> STRIDE) <span style="color:#f92672">%</span> STRIDE
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Apply Reflective Padding</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># &#34;reflec&#34; mirros the pixels at the boundary, preventing edge artifacts</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># that occur with standard Zero-Padding</span>
</span></span><span style="display:flex;"><span>img_padded <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>pad(img_tensor, (<span style="color:#ae81ff">0</span>, pad_W, <span style="color:#ae81ff">0</span>, pad_H), <span style="color:#e6db74">&#34;reflect&#34;</span>)
</span></span></code></pre></div></li>
<li>
<p><strong>Patching Strategy:</strong> (Sliding window technique (128x128 kernels) to handle high-res images)</p>
<p>Instead of processing the full fingerprint at once, the system slices the tensor $ I \in \mathbb{R}^{1 \times H \times W} $ into overlapping patches using PyTorch&rsquo;s <code>unfold</code> operation.</p>
<ul>
<li><strong>Kernel Size:</strong> 128 x 128 pixels.</li>
<li><strong>Stride:</strong> 128 pixels (during training) or overlapping with weighted averaging (during inference).</li>
</ul>
<p>This transformation converts our dataset from $ N $ large, global images into $ N \times K $ smaller, focused texture patches. This forces the Generator to learn <strong>local ridge characteristics</strong> (micro-features) rather than trying to memorize global finger shapes. This makes the watermark robust across different sensor types and resolutions.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Sliding Window Extraction (Unfold)</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># &#34;unfold&#34; extracts sliding local blocks from a large image tensor</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Input: [1, Channels, Height, Width] -&gt; Output: [1, Channels*128*128, Num_Patches]</span>
</span></span><span style="display:flex;"><span>patches <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>unfold(img_padded, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Reshape into Batch Dimension</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># We transpose and reshape so PyTorch treats every patch as an independent sample</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># New Shape: [Num_Patches, Channels, 128, 128]</span>
</span></span><span style="display:flex;"><span>patches <span style="color:#f92672">=</span> patches<span style="color:#f92672">.</span>permute(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">1</span>)<span style="color:#f92672">.</span>contiguous()<span style="color:#f92672">.</span>view(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, C, <span style="color:#ae81ff">128</span>, <span style="color:#ae81ff">128</span>)
</span></span></code></pre></div></li>
<li>
<p><strong>Latent Feature Fusion:</strong> (How a 32-bit text string is expanded to match 2D feature maps)</p>
<p>A key architectural challenge was feeding a 1D text string (the payload) into a 2D Convolutional Network. A naive approach, such as using a Dense layer to flatten the image, destroys spatial correspondence. Instead, I used <strong>Latent Feature Fusion:</strong></p>
<ul>
<li>
<p><strong>Projection:</strong> The 32-bit binary message vector is projected to a higher dimension (e.g., 128 channels) using a fully connected layer.</p>
</li>
<li>
<p><strong>Broadcasting:</strong> This vector is then &ldquo;stretched&rdquo; (repeated) across the spatial dimensions (16 x 16) to match the feature map size at the Generator&rsquo;s bottleneck.</p>
</li>
</ul>
<p>By boradcasting the message, <strong>every pixel in the latent space &ldquo;knows&rdquo; the secret message</strong>. This provides masssive redundancy. It allows the Generator to hide bits locally in every region of the fingerprint. Consequently, even if the final image is cropped or partially obscured, the message can often still be recovered from the remaining fragments.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Define a prjectino layer to expand the 32-bit message</span>
</span></span><span style="display:flex;"><span>self<span style="color:#f92672">.</span>msg_proc <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Sequential(
</span></span><span style="display:flex;"><span>    nn<span style="color:#f92672">.</span>Linear(MESSAGE_LENGTH, <span style="color:#ae81ff">16</span><span style="color:#f92672">*</span><span style="color:#ae81ff">16</span><span style="color:#f92672">*</span><span style="color:#ae81ff">16</span>),
</span></span><span style="display:flex;"><span>    nn<span style="color:#f92672">.</span>ReLU()
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Execution</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Project and reshape the 1D message into a 3D feature volume</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># InputL [Batch, 32] -&gt; Ouput: [Batch, 16, 16, 16]</span>
</span></span><span style="display:flex;"><span>m <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>msg_proc(msg)<span style="color:#f92672">.</span>view(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">16</span>, <span style="color:#ae81ff">16</span>, <span style="color:#ae81ff">16</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Fuse (Concatenate) witht the image bottleneck features</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># IMage Features (128 ch) + Message volume (16 ch) = 144 Channels</span>
</span></span><span style="display:flex;"><span>x <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cat([e3, m], dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span></code></pre></div></li>
</ul>
<h2 id="the-generator-architecture">The Generator Architecture<a hidden class="anchor" aria-hidden="true" href="#the-generator-architecture">#</a></h2>
<p>The Generator handles the writing process. Its job is to take the normalized fingerprint and the message tensor, then output a watermarked image that looks identical to the original.</p>
<p>I chose a <strong>U-Net</strong> architecture for this. While ResNets are standard for classification, they often fail at <strong>pixel-perfect reconstruction</strong>. U-Nets were originally desgined for biomedical segmentaion, which makes them ideal for preserving the fine, high-frequency structures found in fingerprints.</p>
<ul>
<li>
<p><strong>Why Skip Connections Matter:</strong> (Skip Connections (Why they are essential for ridge preservation).)</p>
<p>In a standard Encoder-Decoder, the image is compressed down to a tiny bottleneck and then reconstructed. During this downsampling, fine spatial details, specifically <strong>minutiae points</strong> (ridge endings and bifurications) are often lost.</p>
<p>If the Generator blurs a ridge ending even slightly, the biometric scanner will reject the user.</p>
<p>To fix this, I used <strong>SKip Connections</strong>.</p>
<ul>
<li><strong>Mechanism:</strong> These are direct links connecting the Encoder layers to the corresponding Decoder layers.</li>
<li><strong>Result:</strong> The network passes high-resolution spatial information (the ridge structure) from the input directly to the output, bypassing the bottleneck entirely.</li>
<li><strong>Benefit:</strong> The bottleneck doesn&rsquo;t have to memorize the entire image. It only needs to calculate the <strong>residual noise</strong> (the &ldquo;delta&rdquo;) required to embed the watermark.</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># The U-Net structure allows us to pass spatial details across the network</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>d1 <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>dec1(x)
</span></span><span style="display:flex;"><span>d1_skip <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cat([d1, e2], dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>) <span style="color:#75715e"># Skip connection from Encoder 2</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>d2 <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>dec2(d1_skip)
</span></span><span style="display:flex;"><span>d2_skip <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cat([d2, e1], dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>) <span style="color:#75715e"># Skip connection from Encdoder 1</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># The final layer generates the &#34;noise&#34; (residual) to be added</span>
</span></span><span style="display:flex;"><span>residual <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>dec3(d2_skip)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># The final reuslt is the original image + the generated residual</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">return</span> torch<span style="color:#f92672">.</span>clamp(img <span style="color:#f92672">+</span> residual, <span style="color:#f92672">-</span><span style="color:#ae81ff">1.0</span>, <span style="color:#ae81ff">1.0</span>)
</span></span></code></pre></div></li>
<li>
<p><strong>Bottleneck: Feature Fusion</strong> (Where the Message Tensor is fused with Image Features)</p>
<p>The center of the U-Net is where the visual data meets the text data.</p>
<ol>
<li>
<p><strong>Concatenation:</strong> The Encoder produces a feature map of shape ($ 128 \times 16 \times 16 $). The spatially replicated message tensor (also $ 16 \times 16 $) is concatenated along the channel dimension.</p>
</li>
<li>
<p><strong>Residual Processing:</strong> This combined tensor passes through a series of <strong>Residual Blocks (ResBlocks)</strong>.</p>
<ul>
<li><em>Reasoning</em>: Stacking standard convolution layers here causes signal degradation. ResBLocks allow the network to make complex decisions about where to place the bits. The model learns to hide data in complex texture regions (where ridges intersect) rather than in smooth valleys, making the watermark harder to detect visually.</li>
</ul>
</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">ResBlock</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    Residual Block used in the bottleneck
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    Allows the model to learn complex mappings without vanishing gradient
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">__init__</span>(self, channels):
</span></span><span style="display:flex;"><span>        super(ResBlock, self)<span style="color:#f92672">.</span><span style="color:#a6e22e">__init__</span>()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>block <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Sequential(
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Conv2d(channels, channels, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>BatchNorm2d(channels),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>ReLU(inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Conv2d(channels, channels, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>BatchNorm2d(channels)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> x <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>block(x) <span style="color:#75715e"># The residual connection</span>
</span></span></code></pre></div></li>
<li>
<p><strong>Output Layer:</strong> (Tanh activation for pixel generation)</p>
<p>The Decoder upsamples the features back to the original resolution ($ 128 \times 128 $) using <strong>Transposed Convolution</strong>. The final layer maps the 64 feature channels down to 1 channel (Grayscale) and applies a <strong>Tanh</strong> activation function.</p>
<ul>
<li><strong>Upsampling:</strong> Unlike standard interpolation, Transposed Convolutions utilize learnable weights to &ldquo;paint&rdquo; the pixels back in, ensuring the generated ridge details are sharp.</li>
<li><strong>Consraint:</strong> Tanh bounds the pixel output strictly to [-1, 1]</li>
<li><strong>Consistency:</strong> This matches the input normalization from the preprocessing step, ensuring the Generator produces valid pixel values without needing to clip the data, which would destroy information.</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># The Decoder layers (Upsampling + Channel Reduction)</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Input from Bottleneck: 144 channels (16x16)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 1. Upsample 16x16 -&gt; 32x32</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Input: 144 channels. Output: 64 channels.</span>
</span></span><span style="display:flex;"><span>self<span style="color:#f92672">.</span>dec1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Sequential(
</span></span><span style="display:flex;"><span>    nn<span style="color:#f92672">.</span>ConvTranspose2d(<span style="color:#ae81ff">144</span>, <span style="color:#ae81ff">64</span>, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">4</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>), 
</span></span><span style="display:flex;"><span>    nn<span style="color:#f92672">.</span>ReLU()
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 2. Upsample 32x32 -&gt; 64x64</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Input: 128 channels (64 from dec1 + 64 from skip connection). Output: 32.</span>
</span></span><span style="display:flex;"><span>self<span style="color:#f92672">.</span>dec2 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Sequential(
</span></span><span style="display:flex;"><span>    nn<span style="color:#f92672">.</span>ConvTranspose2d(<span style="color:#ae81ff">128</span>, <span style="color:#ae81ff">32</span>, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">4</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>), 
</span></span><span style="display:flex;"><span>    nn<span style="color:#f92672">.</span>ReLU()
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 3. Output Layer: Upsample 64x64 -&gt; 128x128</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Input: 64 channels (32 from dec2 + 32 from skip connection). Output: 1.</span>
</span></span><span style="display:flex;"><span>self<span style="color:#f92672">.</span>dec3 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Sequential(
</span></span><span style="display:flex;"><span>    nn<span style="color:#f92672">.</span>ConvTranspose2d(<span style="color:#ae81ff">64</span>, <span style="color:#ae81ff">1</span>, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">4</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>), 
</span></span><span style="display:flex;"><span>    nn<span style="color:#f92672">.</span>Tanh() <span style="color:#75715e"># Final activation: [-1, 1] range</span>
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div></li>
</ul>
<h2 id="the-extractor-architecture">The EXtractor Architecture<a hidden class="anchor" aria-hidden="true" href="#the-extractor-architecture">#</a></h2>
<p>The Extractor acts as the &ldquo;Reader&rdquo;. Its sole purpose is to examine a watermarked fingerprint patch ($ 128 \times 128 $) and recover the original 32-bit binary payload.</p>
<p>While the Generator is a complex U-Net, the Extractor is a focused <strong>Convolutional Neural Network (CNN)</strong> designed for multi-label classification.</p>
<ul>
<li>
<p><strong>Backbone: Hierarchial Feature Extraction</strong> (Standard CNN Classifier)</p>
<p>The network follows a standard deep CNN architecture to progessively reduce spatial dimensions while increasing feature depth.</p>
<ul>
<li><strong>layers:</strong> A stacj if 4 Convolutional Blocks.</li>
<li><strong>Components:</strong> Each block consists of <code>Conv2d</code> $\rightarrow$ <code>BatchNorm2d</code> $\rightarrow$ <code>LeakyRelU</code>.</li>
<li><strong>Function:</strong> The early layers detect low-level texture variations (edges, ridges), while deeper layers aggregate these into high-level abstract features representing the hidden signal. The final spatial map is flattened into a dense vector.</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">Extractor</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">__init__</span>(self):
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span><span style="color:#a6e22e">__init__</span>()
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Standard CNN Architecture</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Progressively downsamples 128x128 image to 8x8 feature map</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>main <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Sequential(
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Conv2d(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">32</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">1</span>), nn<span style="color:#f92672">.</span>LeakyReLU(<span style="color:#ae81ff">0.2</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Conv2d(<span style="color:#ae81ff">32</span>, <span style="color:#ae81ff">64</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">1</span>), nn<span style="color:#f92672">.</span>BatchNorm2d(<span style="color:#ae81ff">64</span>), nn<span style="color:#f92672">.</span>LeakyReLU(<span style="color:#ae81ff">0.2</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Conv2d(<span style="color:#ae81ff">64</span>, <span style="color:#ae81ff">128</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">1</span>), nn<span style="color:#f92672">.</span>BatchNorm2d(<span style="color:#ae81ff">128</span>), nn<span style="color:#f92672">.</span>LeakyReLU(<span style="color:#ae81ff">0.2</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Conv2d(<span style="color:#ae81ff">128</span>, <span style="color:#ae81ff">256</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">1</span>), nn<span style="color:#f92672">.</span>BatchNorm2d(<span style="color:#ae81ff">256</span>), nn<span style="color:#f92672">.</span>LeakyReLU(<span style="color:#ae81ff">0.2</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Flatten()
</span></span><span style="display:flex;"><span>        )
</span></span></code></pre></div></li>
<li>
<p><strong>The Noise Layer:</strong> (Differentiable augmentations (Gaussian noise/blur) injected during training to force robust encoding)
The defining feature of the Extractor is <strong>how it is trained</strong>. In a naive steganogaraphy setup, if you feed clean data from the Generator directly to the Extractot, the Generator learns to hide data in the <strong>Least Singificant Bits (LSB)</strong> of the pixels.</p>
<ul>
<li>
<p><strong>Problem:</strong> LSB watermarking is invisible but fragile. It breaks instantly if the image is scanned, compressed (JPEG), or blurred.</p>
</li>
<li>
<p><strong>Solution:</strong> I implemented a <strong>Differentiable Distortion Layer</strong> inside the training loop. $$ I_{input} = G(I) + N(0, \sigma_{noise}) $$</p>
</li>
<li>
<p><strong>Result:</strong> During training, the Extractor never sees a &ldquo;clean&rdquo; watermark. It is forced to read the message through a storm of Gaussian noise. Because the noise is differentiable, gradients flow thorugh it. The Extractor effectively tells the Generator via backpropagation: &ldquo;<em>I can&rsquo;t read this signal when it&rsquo;s noisy; write it louder and deeper</em>&rdquo;.</p>
</li>
</ul>
<p>This forces the GEnerator to abandon fragile LSB tactics and instead encode the watermark into robust, <strong>low-frequency spatial features</strong> that survive real-worl degradation.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Generate Fake Image</span>
</span></span><span style="display:flex;"><span>fake_img <span style="color:#f92672">=</span> generator(real_img, msg)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Differentiable Augmentation</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># We inject random noise BEFORE the Extractor sees the image</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># This forces the Generator to learn robust features that survive corruption</span>
</span></span><span style="display:flex;"><span>noise <span style="color:#f92672">=</span> (torch<span style="color:#f92672">.</span>rand_like(fake_img) <span style="color:#f92672">-</span> <span style="color:#ae81ff">0.5</span>) <span style="color:#f92672">*</span> <span style="color:#ae81ff">0.4</span> <span style="color:#75715e"># High intensity noise</span>
</span></span><span style="display:flex;"><span>fake_img_noisy <span style="color:#f92672">=</span> (fake_img <span style="color:#f92672">+</span> noise)<span style="color:#f92672">.</span>clamp(<span style="color:#f92672">-</span><span style="color:#ae81ff">1.0</span>, <span style="color:#ae81ff">1.0</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Extract from Noisy Image</span>
</span></span><span style="display:flex;"><span>logits <span style="color:#f92672">=</span> extractor(fake_img_noisy)
</span></span></code></pre></div></li>
<li>
<p><strong>The Output head:</strong></p>
<p>The flattened feature vector passes through Multi-layer Perceptron (MLP) ending in a final Linear layer with 32 output neurons.</p>
<ul>
<li><strong>Activation:</strong> Unlike a multiclass classifier (which used Softmax), we use Sigmoid.</li>
<li><strong>Reasoning:</strong> This treats the problem as <strong>Multi-Label Classification</strong>. Each of the 32 bits is an independent Bernoulli variable (0 or 1). The network outputs a probability for each bit position independently, ensuring high accuracy even if one of the string is harder to decode than another.</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># The classification Head</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Input: Flattened features (256 * 8 * 8)</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Output: 32 independent probabilities</span>
</span></span><span style="display:flex;"><span>self<span style="color:#f92672">.</span>head  <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Sequential(
</span></span><span style="display:flex;"><span>    nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">256</span><span style="color:#f92672">*</span><span style="color:#ae81ff">8</span><span style="color:#f92672">*</span><span style="color:#ae81ff">8</span>, <span style="color:#ae81ff">1024</span>), nn<span style="color:#f92672">.</span>ReLU(),
</span></span><span style="display:flex;"><span>    nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">1024</span>, <span style="color:#ae81ff">256</span>), nn<span style="color:#f92672">.</span>ReLU(),
</span></span><span style="display:flex;"><span>    nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">256</span>, <span style="color:#ae81ff">32</span>) <span style="color:#75715e"># The 32-bit(MESSAGE_LENGTH) payload</span>
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Note: We use BCEWithLogitsLoss during training, </span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># which applies Sigmoid activation numerically stably.</span>
</span></span></code></pre></div></li>
</ul>
<h2 id="the-critic-architecture">The Critic Architecture<a hidden class="anchor" aria-hidden="true" href="#the-critic-architecture">#</a></h2>
<p>The Critc serves as the &ldquo;Judge&rdquo;. Its objective is to distinguish between original fingerprints and watermarked fingerprints. Unlike a standard GAN Discriminator which classifies inputs as &ldquo;Real&rdquo; or &ldquo;Fake&rdquo; (0 or 1), the Critic estimates the <strong>Earth Mover&rsquo;s Distance (Wasserstein Distance)</strong> between the data distributions.</p>
<p>I implemented this using the <strong>Wasserstein GAN with Gradient Penalty (WGAN-GP)</strong> framework to prevent the mode collapse and vanishing gradient problems common in standard GANs.</p>
<ul>
<li>
<p><strong>The Architecture:</strong> (Wasserstein GAN with Gradient Penalty (WGAN-GP))</p>
<p>The Critic is a strided Convolutional Neural Network designed to aggressively downsample spatial information into a scalar score.</p>
<ul>
<li><strong>Layers:</strong> 3 blocks of <code>Conv2d</code> (kernel 4, stride 2) $\rightarrow$ <code>LeakyReLU</code>.</li>
<li><strong>Design Choice: No Batch Normalization.</strong> WGAN-GP penalizes the norm of the gradient with respect to each input independently. Batch Normalization creates correlations between samples in a batch, which invalidates the theoretical guarantees of the gradient Penalty.</li>
<li><strong>Output:</strong> A single Linear layer with <strong>no acitvation function</strong>. The output is an unbounded scalar value representing the &ldquo;realness&rdquo; score, not a probability.</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">Critic</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">__init__</span>(self):
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span><span style="color:#a6e22e">__init__</span>()
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Strided CNN to Downsample 128x128 -&gt; Scalar Score</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># NOte: No BatchNorm is used, as required by WGAN-GP</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>main <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Sequential(
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Conv2d(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">32</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">1</span>), nn<span style="color:#f92672">.</span>LeakyReLU(<span style="color:#ae81ff">0.2</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Conv2d(<span style="color:#ae81ff">32</span>, <span style="color:#ae81ff">64</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">1</span>), nn<span style="color:#f92672">.</span>LeakyReLU(<span style="color:#ae81ff">0.2</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Conv2d(<span style="color:#ae81ff">64</span>, <span style="color:#ae81ff">128</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">1</span>), nn<span style="color:#f92672">.</span>LeakyReLU(<span style="color:#ae81ff">0.2</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Flatten(),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">128</span> <span style="color:#f92672">*</span> <span style="color:#ae81ff">16</span> <span style="color:#f92672">*</span> <span style="color:#ae81ff">16</span>, <span style="color:#ae81ff">1</span>) <span style="color:#75715e"># Outputs raw score (unbounded)</span>
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>main(x)
</span></span></code></pre></div></li>
<li>
<p><strong>The Wasserstein Objective:</strong></p>
<p>Standard GANs use Binary Cross Entropy (BCE) loss, which minimizes the Jensen-Shannon divergence. This metric fails when the distributions of Real and Fake images do not overlap significantly, causing gradients to vanish and training to stall.</p>
<p>The Wasserstein metric provides a menaningful gradient everywhere, even when the generated images are poor quality.</p>
<ul>
<li><strong>Mechanism:</strong> The Critic tries to maximize the difference between scores for real images $D(x)$ and fake images $D(G(z))$.</li>
<li><strong>Result:</strong> This creates a linear gradient landscape, allowing the Generator to learn continuously without plateauing.</li>
</ul>
</li>
<li>
<p><strong>Gradient Penalty (1-Lipschitz Constraint):</strong></p>
<p>For the Wasserstein mathematical theory to hold, the Critic must satisfy the <strong>1-Lipschitz constraint</strong> (the rate of change of the function cannot exceed 1).</p>
<ul>
<li><strong>Old Method:</strong> Weight CLipping (forcing weights to be between -0.01 and 0.01). This often leads to exploding or vanishing gradients.</li>
<li><strong>My Implementation: Gradient Penalty.</strong> I sample random points along the straight ines connecting real and fake images. I then penalize the model if the gradient norm at these points deviates from 1. $$ L_{GP} = \mathbb{E} [( \parallel \nabla_{\hat{x}}D(\hat{x}) \parallel_{2} - 1 )^{2}] $$</li>
<li><strong>Outcome:</strong> This ensures stable training dynamics and forces the Generator to match the high-frequency texture statistics of the real fingerprints, rather than just their general shape.</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">gradient_penalty</span>(critic, real, fake, device<span style="color:#f92672">=</span>DEVICE):
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 1. Random Weighting (Interpolation)</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Sample random points along the line connecting Real and Fake images</span>
</span></span><span style="display:flex;"><span>alpha <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>rand(real<span style="color:#f92672">.</span>size(<span style="color:#ae81ff">0</span>), <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>, device<span style="color:#f92672">=</span>device)
</span></span><span style="display:flex;"><span>interpolates <span style="color:#f92672">=</span> (alpha <span style="color:#f92672">*</span> real <span style="color:#f92672">+</span> (<span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> alpha) <span style="color:#f92672">*</span> fake)<span style="color:#f92672">.</span>requires_grad_(<span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 2. Calculate Critic Scores</span>
</span></span><span style="display:flex;"><span>d_interpolates <span style="color:#f92672">=</span> critic(interpolates)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 3. Calculate Gradients w.r.t. Inputs</span>
</span></span><span style="display:flex;"><span>grads <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>autograd<span style="color:#f92672">.</span>grad(
</span></span><span style="display:flex;"><span>    outputs<span style="color:#f92672">=</span>d_interpolates,
</span></span><span style="display:flex;"><span>    inputs<span style="color:#f92672">=</span>interpolates,
</span></span><span style="display:flex;"><span>    grad_outputs<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>ones_like(d_interpolates),
</span></span><span style="display:flex;"><span>    create_graph<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>,
</span></span><span style="display:flex;"><span>    retain_graph<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>,
</span></span><span style="display:flex;"><span>    only_inputs<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>
</span></span><span style="display:flex;"><span>)[<span style="color:#ae81ff">0</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 4. Enforce 1-Lipschitz Constraint</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Penalize if gradient norm moves away from 1.0</span>
</span></span><span style="display:flex;"><span>grads <span style="color:#f92672">=</span> grads<span style="color:#f92672">.</span>view(grads<span style="color:#f92672">.</span>size(<span style="color:#ae81ff">0</span>), <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>gp <span style="color:#f92672">=</span> ((grads<span style="color:#f92672">.</span>norm(<span style="color:#ae81ff">2</span>, dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>) <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>) <span style="color:#f92672">**</span> <span style="color:#ae81ff">2</span>)<span style="color:#f92672">.</span>mean()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">return</span> gp
</span></span></code></pre></div></li>
</ul>
<h2 id="the-biometric-matcher">The Biometric Matcher<a hidden class="anchor" aria-hidden="true" href="#the-biometric-matcher">#</a></h2>
<p>The Biometric Matcher serves as the &ldquo;Security Guard&rdquo; of the architecture. Its sole responsibility is ot enforce the constraint that the watermarked fingerprint ($I_{wm}$) must be accepted by authentication systems as the same identity as the original fingerprint ($I_{orig}$).</p>
<p>To achieve this, I utilized <strong>DeepPrint</strong>, a state-of-the-art fixed-length fingerprint extractor based on the <strong>Inception-ResNet-v2</strong> backbone.</p>
<ul>
<li>
<p><strong>The Architecture: DeepPrint</strong></p>
<p>Unlike traditional minutiae extractors (which output a vairable list of points), DeepPrint maps a fingerprint image to a fixed-size <strong>Identity Vector</strong> (embedding) in a high-dimensional hyperspace (typically $\mathbb{R}^{192}$ or $\mathbb{R}^{256}$).</p>
<ul>
<li><strong>Input:</strong> $299 \times 299$ Grayscale image.</li>
<li><strong>Output:</strong> A dense feature vector representing the unique ridge topology.</li>
<li><strong>Training State:Frozen</strong>. The weights of this model are pre-trained on millions of identities and are <strong>not updated</strong> during our training loop. We treat the Matcher as a differentiable &ldquo;black box&rdquo; judge.</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">Matcher</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">__init__</span>(self, weights_path<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>):
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span><span style="color:#a6e22e">__init__</span>()
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Load the Inception_ResNet backbone</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>deepprint_model <span style="color:#f92672">=</span> DeepPrint_Tex(num_fingerprints<span style="color:#f92672">=</span><span style="color:#ae81ff">8000</span>, texture_embedding_dims<span style="color:#f92672">=</span><span style="color:#ae81ff">256</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># FREEZE WEIGHTS</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># We do not train the matcher; we only use it to calculate loss</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>deepprint_model<span style="color:#f92672">.</span>eval()
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> param <span style="color:#f92672">in</span> self<span style="color:#f92672">.</span>deepprint_model<span style="color:#f92672">.</span>parameters():
</span></span><span style="display:flex;"><span>            param<span style="color:#f92672">.</span>requires_grad <span style="color:#f92672">=</span> <span style="color:#66d9ef">False</span>
</span></span></code></pre></div></li>
<li>
<p><strong>The Mechanism: Cosine Similarity Loss</strong></p>
<p>To quantify &ldquo;Identity Preservation&rdquo;, we cannot simply compare pixel values (L2 loss), as two images can have different pixels but the same identity (e.g., different lightning). Instead, we compare their feature vectors.</p>
<p>I implemented the <strong>Cosine Embedding Loss</strong> to maximize the similarity between the Original and Watermarked vectors.</p>
<ol>
<li>
<p><strong>Forward Pass:</strong> $$ v_{orig} = M(I_{orig}) $$ $$ v_{wm} = M(G(I_{orig}, msg)) $$</p>
<p>However, in production, the <strong>DeepPrint</strong> library has a known bug where it crashes when processing a single image (Batch Size = 1). Since users upload one file at a time, I had to workaround in the forward method to duplicate the tensor on the fly:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># REsize to 299x299 (DeepPrint requirement)</span>
</span></span><span style="display:flex;"><span>    x_resized <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>interpolate(x, size<span style="color:#f92672">=</span>(<span style="color:#ae81ff">299</span>, <span style="color:#ae81ff">299</span>), mode<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;bilinear&#39;</span>, align_corners<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># FIx:</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># DeepPrint library crashes on single images (Batch=1)</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># We duplicate the tensor to fake a batch size of 2 to allow inference</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> x_resized<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>] <span style="color:#f92672">==</span> <span style="color:#ae81ff">1</span>:
</span></span><span style="display:flex;"><span>        x_batch <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cat([x_resized, x_resized], dim<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>        output <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>deepprint_model(x_batch)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> output<span style="color:#f92672">.</span>texture_embeddings[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>unsqueeze(<span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>deepprint_model(x_resized)<span style="color:#f92672">.</span>texture_embeddings
</span></span></code></pre></div></li>
<li>
<p><strong>Loss Calculation:</strong> We calculate the cosine of the angle between the two vectors. $$ Similarity = \frac{v_{orig} . v_{wm}}{\parallel v_{orig} \parallel \parallel v_{wm} \parallel} $$</p>
<p>The loss function minimizes the distance (maximizing similarity): $$ L_{bio} = 1 - CosineSimilarity(v_{orig}, v_{wm}) $$</p>
</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">calculate_biometric_score</span>(matcher, img_orig, img_wm):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">with</span> torch<span style="color:#f92672">.</span>no_grad():
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Fet Identity Vectors (Embeddings)</span>
</span></span><span style="display:flex;"><span>        vec_orig <span style="color:#f92672">=</span> matcher(img_orig)
</span></span><span style="display:flex;"><span>        vec_wm <span style="color:#f92672">=</span> matcher(img_wm)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Calculate COsine Similarity</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Score range: [-1, 1]. we aim for &gt; 0.90</span>
</span></span><span style="display:flex;"><span>        score <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>cosine_similarity(vec_orig, vec_wm, dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)<span style="color:#f92672">.</span>mean()<span style="color:#f92672">.</span>item()
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> score
</span></span></code></pre></div><p><strong>Why Cosine Similarity?</strong> In high-dimensional deep learning spaces, the magnitude of the vector often represents image quality/contrast, while the direction(angle) represents the semantic identity. By forcing the angles to align, we ensure the identity remains constant regardless of local pixel perturbations.</p>
</li>
<li>
<p><strong>Gradient Flow: The &ldquo;Feature Inversion</strong></p>
<p>The most critical concept here is <strong>Backpropagation through a Frozen Network</strong>.</p>
<p>Even though the Matcher&rsquo;s weights as static (<code>requires_grad=False</code>), the network remains part of the computational graph.</p>
<ol>
<li>When $L_{bio}$ is calculated, the error signal is generated at the output of the Matcher.</li>
<li>This gradient flows <strong>backwards</strong> through the layers of the Inception-ResNet.</li>
<li>It reaches the input pixels of the watermarked image ($I_{wm}$).</li>
<li>Since $I_{wm}$ was created by the Generator, the gradients continue flowing into the Generator&rsquo;s weights.</li>
</ol>
<p><strong>Result:</strong> The Generator receives precise, high-level feedback. It is essentially told: &ldquo;<em>You modified a ridge at (x,y) that pushed the Identity Vector 5 degrees off-target. Fix it.</em>&rdquo; This forces the Generator to perform a learned <strong>Feature Inversion</strong>, reconstructing pixels that satisfy the biometric constraints.</p>
</li>
<li>
<p><strong>The &ldquo;Null Space&rdquo; Hypothesis</strong></p>
<p>By simultaneously minimizing BIometric Loss ($L_{bio}$) and maximizing Data Extraction ($L_{extract}$), the Generator implicitly learns to identify the <strong>Null Space</strong> of the fingerprint features.</p>
<ul>
<li>
<p><strong>Null Space:</strong> Regions or frequency nands in the image that do not contribute to the identity vector (e.g., texture between ridges, minor pore shape variations).</p>
</li>
<li>
<p><strong>Outcome:</strong> The Generator learns to hide the payload specifically in these &ldquo;safe zones&rdquo;, leaving the critical minutiae points (used for matching) statistically intact.</p>
</li>
</ul>
</li>
</ul>
<h2 id="the-phases-training-curriculum">The Phases Training Curriculum<a hidden class="anchor" aria-hidden="true" href="#the-phases-training-curriculum">#</a></h2>
<p>Training a multi-network system end-to-end from scratch often leads to failure. If the Generator produces garbage output, the Extractor cannot learn to read. If the Extractor outputs random guesses, the Generator receives no meaningful gradient signal to improve.</p>
<p>To resolve this dependency loop, I implemented a four-stage curriculum that progressively increases task complexity.</p>
<ul>
<li>
<p><strong>Phase 1: Extractor Warmup</strong></p>
<p><strong>Objective:</strong> Solve the &ldquo;Cold Start&rdquo; problem for the Reader. Before the Generator is even initialized, the Extractor must learn to detect binary patterns in image textures.</p>
<ul>
<li>
<p><strong>Mechanism:</strong> I trained the Extractor (E) in isolation using <strong>Synthetic Data</strong>. Instead of waiting for the GEnerator, I mathematically injected the watermark signal into real fingerprints using a simple additive formula: $$ I_{synthetic} = I_{real} + (\alpha \times M_{pattern}) + N(0, \sigma) $$</p>
</li>
<li>
<p><strong>Logic:</strong> By creating a rudimentry, high-amplitude systhetic watermark and corrupting it with noise, we force the Extractor to learn the fundamental frequency of the payload.</p>
</li>
<li>
<p><strong>Outcome:</strong> The Extractor achieves &gt;90% bit accuracy on synthetic data, ensuring that when the Generator starts training, it receives meaningful gradients immediately.</p>
</li>
</ul>
</li>
<li>
<p><strong>Phase 2: Generator Mimic</strong></p>
<p><strong>Objective:</strong> Teach the Writer how to draw a fingerprint. If the Generator tries to hide data before it knows how to generate a valid fingerprint, it will collapse into noise.</p>
<ul>
<li>
<p><strong>Mechanism:</strong> The Generator (G) is trained as a pure <strong>Autoencoder</strong>. The adversarial and extraction losses are disabled. $$ L_{mimic} = \lambda_{L1} \parallel G(I) - I\parallel_{1} + \lambda_{VGG}L_{perceptual} $$</p>
</li>
<li>
<p><strong>Logic:</strong> THis forces the Generator to learn the <strong>Identity Mapping</strong> function. It learns to compress and reconstruct ridge bifuricatoins and pore details pixel-perfectly.</p>
</li>
<li>
<p><strong>Outcome:</strong> The Generator produces clean fingerprint images (PSNR &gt; 35dB) that look identical to input but contain no hidden data yet.</p>
</li>
</ul>
</li>
<li>
<p><strong>Phase 3: Joint Training</strong></p>
<p><strong>Objective:</strong> Connect the Writer and Reader. We unlock both networks and connect them. At this stage, there is no Critic (Discriminator), so the training is purely cooperative.</p>
<ul>
<li>
<p><strong>Mechanism:</strong> The Generator minimizes reconstruction loss and extraction loss. The Extractor maximizes bit accuracy.</p>
</li>
<li>
<p><strong>Logic:</strong> Since $E$ is already warmed up (Phase 1), it guides $G$ on how to modify pixels to encode data. $G$ learns to inject the signal, and $E$ fine-tunes itself to read $G&rsquo;s$ specific encoding style.</p>
</li>
<li>
<p><strong>Outcome:</strong> The system works, data is recoverable, but the watermark is often visibile. Without a critic to penalize it, the Generator takes the path of least resistance, creating visible high-frequency artifacts (like checkerboard patterns) because they are easiest for the Extractor to read.</p>
</li>
</ul>
</li>
<li>
<p><strong>Phase 4: Adversarial &amp; Biometric Tuning</strong></p>
<p><strong>Objective:</strong> Enforce Invisibility and Identity. This is the final production phase. We introduce the <strong>Critic</strong> $(C)$ and the <strong>Biometric Matcher</strong> ($M$). The Generator is now subjected to a constrained multi-objective optimization problem, fighting a war on three fronts:</p>
<ol>
<li><strong>Robustness:</strong> Satisfy the Extractor (keep bit accuracy &gt; 99%)</li>
<li><strong>Invisibility:</strong> Fool the Critic (minimize Wasserstein distance to the real data distribution)</li>
<li><strong>Utility:</strong> Satisfy the Matcher (maintain Cosine Similarity &gt; 0.90)</li>
</ol>
<p>The final loss function becomes a weighted equilibrium: $$ L_{total} = \lambda_{adv}L_{WGAN} + \lambda_{bio}L_{Cosine} + \lambda_{img}L_{L1} $$</p>
<p>This curriculum prevents <strong>Mode Collapse</strong> and ensures the model settles into a local minimum where the watermark is invisible to the human eye, statistically similar to sensor noise, yet perfectly readable by the algorithm.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># The Generator must be balance 4 competing goals simultaneously</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Extraction Loss</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Can the Extractor recover the 32-bit payload?</span>
</span></span><span style="display:flex;"><span>e_loss <span style="color:#f92672">=</span> extractor_loss_fn(logits_g, msg_smooth)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Adversarial Loss (invisibility)</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Does the image look real to the Critic?</span>
</span></span><span style="display:flex;"><span>adv_loss <span style="color:#f92672">=</span> <span style="color:#f92672">-</span>critic(fake_g)<span style="color:#f92672">.</span>mean()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Biometric Loss (indentity)</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Does the identity vector match the original? (DeepPrint)</span>
</span></span><span style="display:flex;"><span>match_loss <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>l1_loss(matcher(fake_g), matcher(real_img))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Perceptual Loss (Quality)</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Do the texture statistics match VGG features?</span>
</span></span><span style="display:flex;"><span>p_loss <span style="color:#f92672">=</span> perceptual_loss(real_img, fake_g)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># FInal equilibrium</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># we weigh the losses to prioritize Data Recovery (300.0) and Biometric (75.0)</span>
</span></span><span style="display:flex;"><span>g_loss <span style="color:#f92672">=</span> (e_loss <span style="color:#f92672">*</span> <span style="color:#ae81ff">300.0</span>) <span style="color:#f92672">+</span> (adv_loss <span style="color:#f92672">*</span> <span style="color:#ae81ff">1.0</span>) <span style="color:#f92672">+</span> (match_loss <span style="color:#f92672">*</span> <span style="color:#ae81ff">75.0</span>) <span style="color:#f92672">+</span> (p_loss <span style="color:#f92672">*</span> <span style="color:#ae81ff">200.0</span>)
</span></span></code></pre></div></li>
</ul>
<h2 id="results--metrics">Results &amp; Metrics<a hidden class="anchor" aria-hidden="true" href="#results--metrics">#</a></h2>
<ul>
<li><strong>Visual:</strong> (PSNR/SSIM score)</li>
<li><strong>Biometric:</strong> (Cosine Similarity score (&gt;0.90 threshold))</li>
<li><strong>Recovery:</strong> (Bit Accuracy rate under attack)</li>
</ul>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://lav-aniya.github.io/tags/deep-learning/">Deep Learning</a></li>
      <li><a href="https://lav-aniya.github.io/tags/gans/">GANs</a></li>
      <li><a href="https://lav-aniya.github.io/tags/python/">Python</a></li>
      <li><a href="https://lav-aniya.github.io/tags/security/">Security</a></li>
    </ul>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share BioMarking on x"
            href="https://x.com/intent/tweet/?text=BioMarking&amp;url=https%3a%2f%2flav-aniya.github.io%2fprojects%2fbiomarking%2f&amp;hashtags=DeepLearning%2cGANs%2cPython%2cSecurity">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share BioMarking on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2flav-aniya.github.io%2fprojects%2fbiomarking%2f&amp;title=BioMarking&amp;summary=BioMarking&amp;source=https%3a%2f%2flav-aniya.github.io%2fprojects%2fbiomarking%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share BioMarking on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2flav-aniya.github.io%2fprojects%2fbiomarking%2f&title=BioMarking">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share BioMarking on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2flav-aniya.github.io%2fprojects%2fbiomarking%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share BioMarking on whatsapp"
            href="https://api.whatsapp.com/send?text=BioMarking%20-%20https%3a%2f%2flav-aniya.github.io%2fprojects%2fbiomarking%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share BioMarking on telegram"
            href="https://telegram.me/share/url?text=BioMarking&amp;url=https%3a%2f%2flav-aniya.github.io%2fprojects%2fbiomarking%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share BioMarking on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=BioMarking&u=https%3a%2f%2flav-aniya.github.io%2fprojects%2fbiomarking%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://lav-aniya.github.io/">Lav-Aniya&#39;s Site</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
