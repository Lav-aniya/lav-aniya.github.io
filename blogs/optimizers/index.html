<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>O1 | Lav-Aniya&#39;s Site</title>
<meta name="keywords" content="Optimization, Deep Learning">
<meta name="description" content="An Introduction to Optimization">
<meta name="author" content="Lav-niya">
<link rel="canonical" href="https://lav-aniya.github.io/blogs/optimizers/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.1f97f95668440377e5dcde099c302fc9dfe0232ee0eb1ce62fb6c6720e6e67b7.css" integrity="sha256-H5f5VmhEA3fl3N4JnDAvyd/gIy7g6xzmL7bGcg5uZ7c=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://lav-aniya.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://lav-aniya.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://lav-aniya.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://lav-aniya.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://lav-aniya.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://lav-aniya.github.io/blogs/optimizers/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" integrity="sha384-wcIxkf4k558AjM3Yz3BBFQUbk/zgIYC2R0QpeeYb+TwlBVMrlgLqwRjRtGZiK7ww" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.js" integrity="sha384-hIoBPJpTUs74ddyc4bFZSM1TVlQDA60VBbJS0oA934VSz82sBx1X7kSx2ATBDIyd" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/auto-render.min.js" integrity="sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
            delimiters: [
                {left: '$$', right: '$$', display: true},
                {left: '$', right: '$', display: false},
                {left: "\\begin{equation}", right: "\\end{equation}", display: true},
                {left: "\\begin{equation*}", right: "\\end{equation*}", display: true},
                {left: "\\begin{align}", right: "\\end{align}", display: true},
                {left: "\\begin{align*}", right: "\\end{align*}", display: true},
                {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
                {left: "\\begin{gather}", right: "\\end{gather}", display: true},
                {left: "\\begin{CD}", right: "\\end{CD}", display: true},
            ],
            throwOnError : false
        });
    });
</script>

<meta property="og:url" content="https://lav-aniya.github.io/blogs/optimizers/">
  <meta property="og:site_name" content="Lav-Aniya&#39;s Site">
  <meta property="og:title" content="O1">
  <meta property="og:description" content="An Introduction to Optimization">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="blogs">
    <meta property="article:published_time" content="2025-09-21T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-09-21T00:00:00+00:00">
    <meta property="article:tag" content="Optimization">
    <meta property="article:tag" content="Deep Learning">
      <meta property="og:see_also" content="https://lav-aniya.github.io/blogs/optimizers2/">
      <meta property="og:see_also" content="https://lav-aniya.github.io/blogs/optimizers3/">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="O1">
<meta name="twitter:description" content="An Introduction to Optimization">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Blogs",
      "item": "https://lav-aniya.github.io/blogs/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "O1",
      "item": "https://lav-aniya.github.io/blogs/optimizers/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "O1",
  "name": "O1",
  "description": "An Introduction to Optimization",
  "keywords": [
    "Optimization", "Deep Learning"
  ],
  "articleBody": "What is Optimization? Optimization is the process of finding the best possible solution from all available options. In deep learing, it specifically means adjusting the model’s internal parameters (weights and biases) to minimize its errors.\nIt’s the core engine of learning for a neural network\nWithout optimization, the model would never learn from its mistakes. It would make the same random, incorrect guesses forever. Optimization is the process that turns a model’s errors into learning.\nSo, Why Do We Actually Need It? Well, without optimization, your AI model is the digital equivalent of a rock. It might be a very complex and expensive rock, but it’s a rock nonetheless.\nHere’s why;\nA freshly made model starts out completely clueless. Its internal settings are random. It’s like it’s making predictions by throwing darts at a board while blindfolded. Without optimization, it would never learn from its mistakes.\nYou’d show it a picture of a dog.\nIt would guess: “Triangle?”\nYou’d say: “Wrong!”\nAnd it would say: “Okay.”\n…and then guess “Triangle?” on the next picture of a dog, too. Forever.\nIt would have no mechanism to take the feedback (“You’re wrong!”) and use it to improve. It’s just confidently wrong, all the time.\nSo, we need optimization because it’s a link that connects a model’s errors to its improvement. Without it, you don’t have “AI”.\nNow we can start looking at our first, most basic optimizer, bit wobbly but it tries its best. It’s name is SGD.\nSGD SGD stands for Stochastic Gradient Descent. A bit fancy, right?\nImagine you own a giant warehouse (that’s your dataset with millions of items). Your job is to arrange everything perfectly (find the best model parameters).\nThe “Normal” Way is a Terrible Idea The old-school, “common sense” way to do this would be Batch Gradient Descent. This is saying like:\n“Okay team, I’m going to personally inspect every single item in this entire warehouse. Only after I’ve seen everything and have a perfect, complete picture in my head will I move a single box.”\nSee the problem? It’s super accurate, but you’d be dead of old age before you moved the first box. It’s just too slow for modern datasets.\nSGD’s way SGD is the new intern who hears that plan, laughs, and says, “Nah, that’s dumb. Watch this.”\nHe grabs the very first item he sees (a single, random data point), and immediately makes a decision.\n“This is a fidget spinner! okay, i’m gonna rearrange the whole ‘Anxiety Relief’ aisle based only on this spinner.”\nThen he grabs the next item, a cat meme, and says, “Such funny. Let’s shift the entire ‘Internet Culture’ section over here.”\nThe “Stochastic part of his name just means random. He makes huge decision based on tiny, random pieces of information. It’s a chaotic and impulsive way to work. His path to organizing the warehouse looks less like a plan and more like a drunk person stumbling around.\nHe’s constantly overcorrecting. One moment he’s certain the whole warehouse is full of fidget spinners, the next it’s all cat memes.\nThe Techincal Dive on SGD Let’s look under the hood at the actual machinery. While the analogy of a clumsy intern is fun, the reality is all about the math.\nThe “Perfect” but Slow Way: Batch Gradient Descent (BGD)\nFirst, let’s look at the “vanilla” Gradient Descent, also called Batch Gradient Descent. This is the version that inspects the entire warehouse before moving a single box.\nThe update rule is a single, clean equation:\n$$\\theta = \\theta - \\eta \\cdot \\nabla_{ \\theta} J(\\theta; X,y)$$\nLet’s break down:\n$\\theta$ (theta) is model’s parameters (all the knobs and dials). $=$ means we are updating the parameters. $\\eta$ (eta) is the learning rate, how big of a step we take. $\\nabla_{\\theta}J(\\theta; X, y)$ is the gradient. This is the most important part. It’s the direction of the steepest ascent, calculated by looking at the total error $J$ across the entire dataset ($X$ for all inputs, $y$ for all correct labels). The key takeaway is that to calculate this one gradient, you have to run every single data point through your model. If you have a million data points, this is incredibly slow.\nThe Impulsive Way: Stochastic Gradient Descent (SGD)\nSGD’s strategy is to get a “good enough” direction but do it way faster. Instead of using the whole dataset $X, y$, it picks one single, random data point ($x^{(i)}, y^{(i)}$) and calculates the gradient based on that alone.\nThe update rule looks almost identical, but with a crucial change:\n$$\\theta = \\theta - \\eta \\cdot \\nabla_{ \\theta} J(\\theta; x_{i},y_{i})$$\nThe only difference is that the gradient $\\nabla_{\\theta}J$ is now calculated for a single random sample $i$. This gradient is a noisy, rough estimate of the “true” gradient from BGD, but since we can calculate it thousands of times before BGD finishes even one step, we make progress much faster.\nThe Modern Compromise: Mini-Batch Gradient Descent\nSaddle Point Simple Bowl As we discussed, this is what everyone actually uses. We take a small, random batch of $m$ samples (e.g., $m=32$).\nThe update rule reflects this compromise:\n$$\\theta = \\theta - \\eta \\cdot \\nabla_{ \\theta} J(\\theta; X^{(i:i+m)},y^{(i:i+m)})$$\nHere, the gradient is averaged over a small mini-batch of data ($X^{(i:i+m)}$). This gives us a much better estimate of the true gradient than a single sample, while still being way faster than using the full dataset.\nThe Algorithm in a Nutshell (Pseudocode)\nHere’s what the process looks like in code. This is the heart of every deep learning training loop.\n# model.parameters are all the weights and biases for epoch in range(num_epochs): # Shuffle data for stochasticity shuffle(dataset) for mini_batch in dataset: # 1. Calculate loss and gradient predictions = model.forward(mini_batch.data) loss = loss_function(predictions, mini_batch.labels) gradient = loss.backward() # Backpropagation # 2. Update parameters model.parameters = model.parameters - learning_rate * gradient Key Hyperparameters and Considerations Learning Rate ($\\eta$): This is the big one. Because SGD is so noisy, the learning rate need to be smaller than what you’d use for BGD. A common practice is learning rate scheduling, where you start with a larger $\\eta$ and gradually decrease it over time. This lets the model learn fast at the beginning and then take smaller, more careful steps as it gets closer to the minimum, helping it to stop bouncing around and actually settle.\nBatch Size ($m$): This is a trade-off.\nSmall Batch Size (e.g., 16, 32): Noisy updates, but less memory required. The noise can help the model escape sharp, narrow minima that might not be the best solution anyway.\nLarge Batch Size (e.g., 128, 256): More stable and accurate gradient estimates, but requires more memory and computational power. The updates are smoother.\nSGD with Momentum While SGD is fast, it’s a mess. It gets thrown off by every little thing, and it’s path to the goal is a wobbly, inefficient zig-zag. Especially in long, narrow valleys, it spend most of it’s time bouncing from one wall to the other instead of moving down the valley. We need a way to smooth this out.\nThe Intuitive Idea The problem: SGD has no memory. It’s a goldfish. At every step, it forgets where it was going and only looks at the ground right under its feet to decide on the next move. This makes it jerky and easily distracted.\nThe Big Idea: What if, instead of clumsy person stumbling downhill, we had a heavy ball rolling downhill?\nA heavy ball doesn’t get thrown off by small bumps or pebbles. It has momentum. It builds up speed in the main downhill direction, and its own inertia helps it blast right over the little noisy parts and resist changing direction.\nThis is exactly what the Momentum optimizer does. We’re giving our intern a little bit of “memory” and “weight.”\nHow it Works (In Simple Terms):\nAt every step, we don’t just tell out intern, “Here’s the new direction to go.” Instead, we say:\n“Okay, take the new direction, but also add a big chunk of the direction you were just going.”\nIf the new direction is roughly the same as the old one, the two add up, and the intern speeds up, rolling faster down the consistent slope. If the new direction is opposite to the old one (a classic wobble), they cancel each other out, which dampens the zig-zagging. The result? A much smoother, faster, and more confident path to the bottom.\nSaddle Point [SGD vs SGDM] Simple Bowl [SGD vs SGDM] The Technical Dive The core of Momentum is that we don’t just use the gradient to update our parameters. We maintain a “velocity” vector that gets updated at each step, and we use that velocity to update our model’s weights.\nThe Update Rule:\nThere are two steps:\nCalculate the velocity: $$v_{t} = \\beta v_{t-1} + \\eta \\nabla_{\\theta} J (\\theta)$$\nUpdate the parameters: $$\\theta = \\theta - v{_t}$$\n$v_t$ is the velocity vector at the current time step $t$. Think of this as the direction and speed of our update.\n$\\beta$ (beta) is the momentum coefficient. This is the new, critical hyperparameter. It’s a value between 0 and 1, typically set to something like 0.9. It controls how much of the previous velocity $v_{t-1}$ we “remember” and carry over to the current step.\n$v_{t-1}$ is the velocity from the previous time step.\n$\\eta \\nabla_{\\theta}J(\\theta)$ is the same gradient step we had in SGD.\nWhy It Works:\nThe velocity vector $v_{t}$ is essentially an exponentially weighted moving average (EWMA) of the gradients, its purpose is to calculate a stable average of the recent gradients.\nAcceleration: When the gradient $\\nabla_{\\theta}J(\\theta)$ consistently points in the same direction over several steps, the terms in the velocity equation add up and compound. This makes $v_{t}$ grow larger and larger, effectively accelerating the descent.\nDampening Oscillations: Imagine the model is in a narrow ravine, and the gradient keeps flipping its sign (e.g., +0.5, -0.5, +0.5, ...). Because $v_{t}$ is an average over past steps, these opposing gradients will cancel each other out over time. The velocity in the wobbly direction will shrink, while the velocity in the consistent downhill direction will grow.\nKEY Hyperparameters: Learning Rate ($\\eta$): Still as important as ever.\nMomentum ($\\beta$): This is the new knob to tune. A higher value (like 0.9) means the optimizer has more “memory” and is more resistant to changing its course. If you set $\\beta = 0$, you’re right back to plain old SGD.\nRead the next part of this series: O2\n",
  "wordCount" : "1749",
  "inLanguage": "en",
  "datePublished": "2025-09-21T00:00:00Z",
  "dateModified": "2025-09-21T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Lav-niya"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://lav-aniya.github.io/blogs/optimizers/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Lav-Aniya's Site",
    "logo": {
      "@type": "ImageObject",
      "url": "https://lav-aniya.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://lav-aniya.github.io/" accesskey="h" title="Lav-Aniya&#39;s Site (Alt + H)">Lav-Aniya&#39;s Site</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://github.com/Lav-aniya" title="&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-width=&#34;2&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34;&gt;&lt;path d=&#34;M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22&#34;&gt;&lt;/path&gt;&lt;/svg&gt;">
                    <span><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"></path></svg></span>
                    
                </a>
            </li>
            <li>
                <a href="https://www.linkedin.com/in/your-profile-name/" title="&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-width=&#34;2&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34;&gt;&lt;path d=&#34;M16 8a6 6 0 0 1 6 6v7h-4v-7a2 2 0 0 0-2-2 2 2 0 0 0-2 2v7h-4v-7a6 6 0 0 1 6-6z&#34;&gt;&lt;/path&gt;&lt;rect x=&#34;2&#34; y=&#34;9&#34; width=&#34;4&#34; height=&#34;12&#34;&gt;&lt;/rect&gt;&lt;circle cx=&#34;4&#34; cy=&#34;4&#34; r=&#34;2&#34;&gt;&lt;/circle&gt;&lt;/svg&gt;">
                    <span><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M16 8a6 6 0 0 1 6 6v7h-4v-7a2 2 0 0 0-2-2 2 2 0 0 0-2 2v7h-4v-7a6 6 0 0 1 6-6z"></path><rect x="2" y="9" width="4" height="12"></rect><circle cx="4" cy="4" r="2"></circle></svg></span>
                    
                </a>
            </li>
            <li>
                <a href="mailto:lavaniya.nen@gmail.com" title="&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-width=&#34;2&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34;&gt;&lt;path d=&#34;M4 4h16c1.1 0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1 0-2-.9-2-2V6c0-1.1.9-2 2-2z&#34;&gt;&lt;/path&gt;&lt;polyline points=&#34;22,6 12,13 2,6&#34;&gt;&lt;/polyline&gt;&lt;/svg&gt;">
                    <span><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M4 4h16c1.1 0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1 0-2-.9-2-2V6c0-1.1.9-2 2-2z"></path><polyline points="22,6 12,13 2,6"></polyline></svg></span>
                    
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      O1
    </h1>
    <div class="post-description">
      An Introduction to Optimization
    </div>
    <div class="post-meta"><span title='2025-09-21 00:00:00 +0000 UTC'>September 21, 2025</span>&nbsp;·&nbsp;9 min&nbsp;·&nbsp;Lav-niya

</div>
  </header> 
  <div class="post-content"><h3 id="what-is-optimization">What is Optimization?<a hidden class="anchor" aria-hidden="true" href="#what-is-optimization">#</a></h3>
<p>Optimization is the process of finding the best possible solution from all available options. In deep learing, it specifically means <strong>adjusting the model&rsquo;s internal parameters (weights and biases) to minimize its errors</strong>.</p>
<p>It&rsquo;s the core engine of learning for a neural network</p>
<p>Without optimization, the model would never learn from its mistakes. It would make the same random, incorrect guesses forever. <strong>Optimization is the process that turns a model&rsquo;s errors into learning.</strong></p>
<h3 id="so-why-do-we-actually-need-it">So, Why Do We Actually Need It?<a hidden class="anchor" aria-hidden="true" href="#so-why-do-we-actually-need-it">#</a></h3>
<p>Well, without optimization, your AI model is the digital equivalent of a rock. It might be a very complex and expensive rock, but it&rsquo;s a rock nonetheless.</p>
<p>Here&rsquo;s why;</p>
<p>A freshly made model starts out completely clueless. Its internal settings are random. It&rsquo;s like it&rsquo;s making predictions by throwing darts at a board while blindfolded. Without optimization, it would <strong>never learn from its mistakes.</strong></p>
<p>You&rsquo;d show it a picture of a dog.</p>
<p>It would guess: &ldquo;Triangle?&rdquo;</p>
<p>You&rsquo;d say: &ldquo;Wrong!&rdquo;</p>
<p>And it would say: &ldquo;Okay.&rdquo;</p>
<p>&hellip;and then guess &ldquo;Triangle?&rdquo; on the next picture of a dog, too. Forever.</p>
<p>It would have no mechanism to take the feedback (&ldquo;You&rsquo;re wrong!&rdquo;) and use it to improve. It&rsquo;s just confidently wrong, all the time.</p>
<p>So, we need optimization because it&rsquo;s a link that connects a model&rsquo;s errors to its improvement. Without it, you don&rsquo;t have &ldquo;AI&rdquo;.</p>
<p>Now we can start looking at our first, most basic optimizer, bit wobbly but it tries its best. It&rsquo;s name is SGD.</p>
<h2 id="sgd"><em><strong>SGD</strong></em><a hidden class="anchor" aria-hidden="true" href="#sgd">#</a></h2>
<p>SGD stands for <strong>Stochastic Gradient Descent</strong>. A bit fancy, right?</p>
<p>Imagine you own a giant warehouse (that&rsquo;s your dataset with millions of items). Your job is to arrange everything perfectly (find the best model parameters).</p>
<h4 id="the"><strong>The &ldquo;Normal&rdquo; Way is a Terrible Idea</strong><a hidden class="anchor" aria-hidden="true" href="#the">#</a></h4>
<p>The old-school, &ldquo;common sense&rdquo; way to do this would be <strong>Batch Gradient Descent</strong>. This is saying like:</p>
<p>&ldquo;Okay team, I&rsquo;m going to personally inspect <em>every single item in this entire warehouse</em>. Only after I&rsquo;ve seen everything and have a perfect, complete picture in my head will I move a single box.&rdquo;</p>
<p>See the problem? It&rsquo;s super accurate, but you&rsquo;d be dead of old age before you moved the first box. It&rsquo;s just too slow for modern datasets.</p>
<h3 id="sgds-way">SGD&rsquo;s way<a hidden class="anchor" aria-hidden="true" href="#sgds-way">#</a></h3>
<p>SGD is the new intern who hears that plan, laughs, and says, &ldquo;Nah, that&rsquo;s dumb. Watch this.&rdquo;</p>
<p>He grabs the <strong>very first item</strong> he sees (a single, random data point), and immediately makes a decision.</p>
<p>&ldquo;This is a fidget spinner! okay, i&rsquo;m gonna rearrange the whole &lsquo;Anxiety Relief&rsquo; aisle based <em>only</em> on this spinner.&rdquo;</p>
<p>Then he grabs the next item, a cat meme, and says, &ldquo;Such funny. Let&rsquo;s shift the entire &lsquo;Internet Culture&rsquo; section over here.&rdquo;</p>
<p>The <strong>&ldquo;Stochastic</strong> part of his name just means <strong>random</strong>. He makes huge decision based on tiny, random pieces of information. It&rsquo;s a chaotic and impulsive way to work. His path to organizing the warehouse looks less like a plan and more like a drunk person stumbling around.</p>
<p>He&rsquo;s constantly overcorrecting. One moment he&rsquo;s certain the whole warehouse is full of fidget spinners, the next it&rsquo;s all cat memes.</p>
<p><img alt="A diagram explaining the process" loading="lazy" src="/optimizers/sgd_contour.png"></p>
<h3 id="the-techincal-dive-on-sgd">The Techincal Dive on SGD<a hidden class="anchor" aria-hidden="true" href="#the-techincal-dive-on-sgd">#</a></h3>
<p>Let&rsquo;s look under the hood at the actual machinery. While the analogy of a clumsy intern is fun, the reality is all about the math.</p>
<p><strong>The &ldquo;Perfect&rdquo; but Slow Way: Batch Gradient Descent (BGD)</strong></p>
<p>First, let&rsquo;s look at the &ldquo;vanilla&rdquo; Gradient Descent, also called Batch Gradient Descent. This is the version that inspects the <em>entire</em> warehouse before moving a single box.</p>
<p>The update rule is a single, clean equation:</p>
<p>$$\theta = \theta - \eta \cdot \nabla_{ \theta} J(\theta; X,y)$$</p>
<p>Let&rsquo;s break down:</p>
<ul>
<li>$\theta$ (theta) is model&rsquo;s <strong>parameters</strong> (all the knobs and dials).</li>
<li>$=$ means we are updating the parameters.</li>
<li>$\eta$ (eta) is the <strong>learning rate</strong>, how big of a step we take.</li>
<li>$\nabla_{\theta}J(\theta; X, y)$ is the <strong>gradient</strong>. This is the most important part. It&rsquo;s the direction of the steepest ascent, calculated by looking at the total error $J$ across the <strong>entire dataset</strong> ($X$ for all inputs, $y$ for all correct labels).</li>
</ul>
<p>The key takeaway is that to calculate this one gradient, you have to run every single data point through your model. If you have a million data points, this is incredibly slow.</p>
<p><strong>The Impulsive Way: Stochastic Gradient Descent (SGD)</strong></p>
<p>SGD&rsquo;s strategy is to get a &ldquo;good enough&rdquo; direction but do it way faster. Instead of using the whole dataset $X, y$, it picks <strong>one single, random data point</strong> ($x^{(i)}, y^{(i)}$) and calculates the gradient based on that alone.</p>
<p>The update rule looks almost identical, but with a crucial change:</p>
<p>$$\theta = \theta - \eta \cdot \nabla_{ \theta} J(\theta; x_{i},y_{i})$$</p>
<p>The only difference is that the gradient $\nabla_{\theta}J$ is now calculated for a single random sample $i$. This gradient is a noisy, rough estimate of the &ldquo;true&rdquo; gradient from BGD, but since we can calculate it thousands of times before BGD finishes even one step, we make progress much faster.</p>
<p><strong>The Modern Compromise: Mini-Batch Gradient Descent</strong></p>
<table>
  <thead>
      <tr>
          <th style="text-align: center"><strong>Saddle Point</strong></th>
          <th style="text-align: center"><strong>Simple Bowl</strong></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: center"><img alt="A gif explaining saddle point" loading="lazy" src="/optimizers/gifs/saddlepoint.gif"></td>
          <td style="text-align: center"><img alt="A gif explaining simple bowl" loading="lazy" src="/optimizers/gifs/simplebowl.gif"></td>
      </tr>
  </tbody>
</table>
<p>As we discussed, this is what everyone actually uses. We take a small, random batch of $m$ samples (e.g., $m=32$).</p>
<p>The update rule reflects this compromise:</p>
<p>$$\theta = \theta - \eta \cdot \nabla_{ \theta} J(\theta; X^{(i:i+m)},y^{(i:i+m)})$$</p>
<p>Here, the gradient is averaged over a small mini-batch of data ($X^{(i:i+m)}$). This gives us a much better estimate of the true gradient than a single sample, while still being way faster than using the full dataset.</p>
<p><strong>The Algorithm in a Nutshell (Pseudocode)</strong></p>
<p>Here’s what the process looks like in code. This is the heart of every deep learning training loop.</p>
<pre><code># model.parameters are all the weights and biases
for epoch in range(num_epochs):
    # Shuffle data for stochasticity
    shuffle(dataset)
    for mini_batch in dataset:
        # 1. Calculate loss and gradient
        predictions = model.forward(mini_batch.data)
        loss = loss_function(predictions, mini_batch.labels)
        gradient = loss.backward() # Backpropagation

        # 2. Update parameters
        model.parameters = model.parameters - learning_rate * gradient    
</code></pre>
<h3 id="key-hyperparameters-and-considerations">Key Hyperparameters and Considerations<a hidden class="anchor" aria-hidden="true" href="#key-hyperparameters-and-considerations">#</a></h3>
<ol>
<li>
<p><strong>Learning Rate ($\eta$)</strong>: This is the big one. Because SGD is so noisy, the learning rate need to be smaller than what you&rsquo;d use for BGD. A common practice is <strong>learning rate scheduling</strong>, where you start with a larger $\eta$ and gradually decrease it over time. This lets the model learn fast at the beginning and then take smaller, more careful steps as it gets closer to the minimum, helping it to stop bouncing around and actually settle.</p>
</li>
<li>
<p><strong>Batch Size ($m$)</strong>: This is a trade-off.</p>
</li>
</ol>
<ul>
<li>
<p><strong>Small Batch Size (e.g., 16, 32)</strong>: Noisy updates, but less memory required. The noise can help the model escape sharp, narrow minima that might not be the best solution anyway.</p>
</li>
<li>
<p><strong>Large Batch Size (e.g., 128, 256)</strong>: More stable and accurate gradient estimates, but requires more memory and computational power. The updates are smoother.</p>
</li>
</ul>
<hr>
<h2 id="sgd-with-momentum"><em><strong>SGD with Momentum</strong></em><a hidden class="anchor" aria-hidden="true" href="#sgd-with-momentum">#</a></h2>
<p>While <strong>SGD</strong> is fast, it&rsquo;s a mess. It gets thrown off by every little thing, and it&rsquo;s path to the goal is a wobbly, inefficient zig-zag. Especially in long, narrow valleys, it spend most of it&rsquo;s time bouncing from one wall to the other instead of moving down the valley. We need a way to smooth this out.</p>
<h3 id="the-intuitive-idea">The Intuitive Idea<a hidden class="anchor" aria-hidden="true" href="#the-intuitive-idea">#</a></h3>
<p><strong>The problem</strong>: SGD has no memory. It&rsquo;s a goldfish. At every step, it forgets where it was going and only looks at the ground right under its feet to decide on the next move. This makes it jerky and easily distracted.</p>
<p><strong>The Big Idea</strong>: What if, instead of clumsy person stumbling downhill, we had a <strong>heavy ball rolling downhill?</strong></p>
<p>A heavy ball doesn&rsquo;t get thrown off by small bumps or pebbles. It has <strong>momentum</strong>. It builds up speed in the main downhill direction, and its own inertia helps it blast right over the little noisy parts and resist changing direction.</p>
<p>This is exactly what the Momentum optimizer does. We&rsquo;re giving our intern a little bit of &ldquo;memory&rdquo; and &ldquo;weight.&rdquo;</p>
<p><strong>How it Works (In Simple Terms)</strong>:</p>
<p>At every step, we don&rsquo;t just tell out intern, &ldquo;Here&rsquo;s the new direction to go.&rdquo; Instead, we say:</p>
<p>&ldquo;Okay, take the new direction, but also add a big chunk of the direction you were <em>just</em> going.&rdquo;</p>
<ul>
<li>If the new direction is roughly the same as the old one, the two add up, and the intern <strong>speeds up</strong>, rolling faster down the consistent slope.</li>
<li>If the new direction is opposite to the old one (a classic wobble), they cancel each other out, which <strong>dampens the zig-zagging</strong>.</li>
</ul>
<p>The result? A much smoother, faster, and more confident path to the bottom.</p>
<table>
  <thead>
      <tr>
          <th style="text-align: center"><strong>Saddle Point [SGD vs SGDM]</strong></th>
          <th style="text-align: center"><strong>Simple Bowl [SGD vs SGDM]</strong></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: center"><img alt="A gif explaining SGD vs SGDM in saddle point" loading="lazy" src="/optimizers/gifs/momentumsaddlepoint.gif"></td>
          <td style="text-align: center"><img alt="A gif explaining SGD vs SGDM in simple bowl" loading="lazy" src="/optimizers/gifs/momentumsimplebowl.gif"></td>
      </tr>
  </tbody>
</table>
<h3 id="the-technical-dive">The Technical Dive<a hidden class="anchor" aria-hidden="true" href="#the-technical-dive">#</a></h3>
<p>The core of Momentum is that we don&rsquo;t just use the gradient to update our parameters. We maintain a &ldquo;velocity&rdquo; vector that gets updated at each step, and we use that velocity to update our model&rsquo;s weights.</p>
<p><strong>The Update Rule</strong>:</p>
<p>There are two steps:</p>
<ol>
<li><strong>Calculate the velocity</strong>:</li>
</ol>
<p>$$v_{t} = \beta v_{t-1} + \eta \nabla_{\theta} J (\theta)$$</p>
<ol start="2">
<li><strong>Update the parameters</strong>:</li>
</ol>
<p>$$\theta = \theta - v{_t}$$</p>
<ul>
<li>
<p>$v_t$ is the <strong>velocity vector</strong> at the current time step $t$. Think of this as the direction and speed of our update.</p>
</li>
<li>
<p>$\beta$ (beta) is the <strong>momentum coefficient</strong>. This is the new, critical hyperparameter. It&rsquo;s a value between 0 and 1, typically set to something like <code>0.9</code>. It controls how much of the previous velocity $v_{t-1}$ we &ldquo;remember&rdquo; and carry over to the current step.</p>
</li>
<li>
<p>$v_{t-1}$ is the velocity from the previous time step.</p>
</li>
<li>
<p>$\eta \nabla_{\theta}J(\theta)$ is the same gradient step we had in SGD.</p>
</li>
</ul>
<p><strong>Why It Works</strong>:</p>
<p>The velocity vector $v_{t}$ is essentially an <strong>exponentially weighted moving average (EWMA)</strong> of the gradients, its purpose is to calculate a stable average of the recent gradients.</p>
<ul>
<li>
<p>Acceleration: When the gradient $\nabla_{\theta}J(\theta)$ consistently points in the same direction over several steps, the terms in the velocity equation add up and compound. This makes $v_{t}$ grow larger and larger, effectively accelerating the descent.</p>
</li>
<li>
<p>Dampening Oscillations: Imagine the model is in a narrow ravine, and the gradient keeps flipping its sign (e.g., <code>+0.5, -0.5, +0.5, ...</code>). Because $v_{t}$ is an average over past steps, these opposing gradients will cancel each other out over time. The velocity in the wobbly direction will shrink, while the velocity in the consistent downhill direction will grow.</p>
</li>
</ul>
<h3 id="key-hyperparameters">KEY Hyperparameters:<a hidden class="anchor" aria-hidden="true" href="#key-hyperparameters">#</a></h3>
<ol>
<li>
<p><strong>Learning Rate ($\eta$):</strong> Still as important as ever.</p>
</li>
<li>
<p><strong>Momentum ($\beta$):</strong> This is the new knob to tune. A higher value (like <code>0.9</code>) means the optimizer has more &ldquo;memory&rdquo; and is more resistant to changing its course. If you set $\beta = 0$, you&rsquo;re right back to plain old SGD.</p>
</li>
</ol>
<p>Read the next part of this series: <a href="/blogs/optimizers2/">O2</a></p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://lav-aniya.github.io/tags/optimization/">Optimization</a></li>
      <li><a href="https://lav-aniya.github.io/tags/deep-learning/">Deep Learning</a></li>
    </ul>
<nav class="paginav">
  <a class="next" href="https://lav-aniya.github.io/blogs/optimizers2/">
    <span class="title">Next »</span>
    <br>
    <span>O2</span>
  </a>
</nav>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share O1 on x"
            href="https://x.com/intent/tweet/?text=O1&amp;url=https%3a%2f%2flav-aniya.github.io%2fblogs%2foptimizers%2f&amp;hashtags=Optimization%2cDeepLearning">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share O1 on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2flav-aniya.github.io%2fblogs%2foptimizers%2f&amp;title=O1&amp;summary=O1&amp;source=https%3a%2f%2flav-aniya.github.io%2fblogs%2foptimizers%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share O1 on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2flav-aniya.github.io%2fblogs%2foptimizers%2f&title=O1">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share O1 on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2flav-aniya.github.io%2fblogs%2foptimizers%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share O1 on whatsapp"
            href="https://api.whatsapp.com/send?text=O1%20-%20https%3a%2f%2flav-aniya.github.io%2fblogs%2foptimizers%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share O1 on telegram"
            href="https://telegram.me/share/url?text=O1&amp;url=https%3a%2f%2flav-aniya.github.io%2fblogs%2foptimizers%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share O1 on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=O1&u=https%3a%2f%2flav-aniya.github.io%2fblogs%2foptimizers%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://lav-aniya.github.io/">Lav-Aniya&#39;s Site</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
