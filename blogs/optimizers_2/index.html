<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>O2 | Lav-Aniya&#39;s Site</title>
<meta name="keywords" content="Optimization, Deep Learning">
<meta name="description" content="An Introduction to Optimization">
<meta name="author" content="Lav-niya">
<link rel="canonical" href="https://lav-aniya.github.io/blogs/optimizers_2/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.1f97f95668440377e5dcde099c302fc9dfe0232ee0eb1ce62fb6c6720e6e67b7.css" integrity="sha256-H5f5VmhEA3fl3N4JnDAvyd/gIy7g6xzmL7bGcg5uZ7c=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://lav-aniya.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://lav-aniya.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://lav-aniya.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://lav-aniya.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://lav-aniya.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://lav-aniya.github.io/blogs/optimizers_2/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" integrity="sha384-wcIxkf4k558AjM3Yz3BBFQUbk/zgIYC2R0QpeeYb+TwlBVMrlgLqwRjRtGZiK7ww" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.js" integrity="sha384-hIoBPJpTUs74ddyc4bFZSM1TVlQDA60VBbJS0oA934VSz82sBx1X7kSx2ATBDIyd" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/auto-render.min.js" integrity="sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
            delimiters: [
                {left: '$$', right: '$$', display: true},
                {left: '$', right: '$', display: false},
                {left: "\\begin{equation}", right: "\\end{equation}", display: true},
                {left: "\\begin{equation*}", right: "\\end{equation*}", display: true},
                {left: "\\begin{align}", right: "\\end{align}", display: true},
                {left: "\\begin{align*}", right: "\\end{align*}", display: true},
                {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
                {left: "\\begin{gather}", right: "\\end{gather}", display: true},
                {left: "\\begin{CD}", right: "\\end{CD}", display: true},
            ],
            throwOnError : false
        });
    });
</script>

<meta property="og:url" content="https://lav-aniya.github.io/blogs/optimizers_2/">
  <meta property="og:site_name" content="Lav-Aniya&#39;s Site">
  <meta property="og:title" content="O2">
  <meta property="og:description" content="An Introduction to Optimization">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="blogs">
    <meta property="article:published_time" content="2025-09-22T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-09-22T00:00:00+00:00">
    <meta property="article:tag" content="Optimization">
    <meta property="article:tag" content="Deep Learning">
      <meta property="og:see_also" content="https://lav-aniya.github.io/blogs/optimizers/">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="O2">
<meta name="twitter:description" content="An Introduction to Optimization">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Blogs",
      "item": "https://lav-aniya.github.io/blogs/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "O2",
      "item": "https://lav-aniya.github.io/blogs/optimizers_2/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "O2",
  "name": "O2",
  "description": "An Introduction to Optimization",
  "keywords": [
    "Optimization", "Deep Learning"
  ],
  "articleBody": "This is the second part of the series. You can read the first part here: O1\nWe have seen that SGD is clumsy and Momentum is a bit smoother but still uses a one-size-fits-all approach. Now, let’s meet an optimizer that tries to be a little smarter.\nAdagrad: The Adaptive Gradient Optimizer SGD and Momentum, they are like mechanics who uses the exact same wrench with the exact force on every single bold of an engine. This is a problem. Some bolts are tiny and sensitive, while others are big and rusty. You can’t treat them all the same. Adagrad is the first applicant who walks in with a full, custom toolkit.\nThe Intuitive Idea The Problem: Your model can have millions of parameters (knobs). Some of these knobs might be very important and get updated frequently. Others might control very niche features and are rarely touched. Using the same learning rate (step size) for all of them is inefficient.\nThe Big Idea: Adagrad gives a unique, custom learning rate to every single knob. It adapts the learning rate based on how much that specific knob has turned in the past. This is why its name is Adagrad, which stands for Adaptive Gradient.\nHow it Works:\nAdagrad is like a cautious manager. It keeps a running tally of how “active” each knob has been.\nFor knobs that have seen a lot of action (frequent, large updates), Adagrad gets nervous. It thinks, “Whoa, this knob is really sensitive. Let’s be gentle with it.” It dramatically shrinks the learning rate for that specific knob. For knobs that have barely been touched (infrequent updates, like for rare words in a vocabulary), Adagrad thinks, “This one’s been ignored. Let’s give it a bigger push to see what happens.” It keeps a larger learning rate for that knob. This is a game-changer for problems with sparse data. For example, in language models, the parameter for a common word like “the” will get small updates, while the parameter for a rare word like “aficionado” will get much larger updates, helping it learn effectively even though it appears less often.\nThe Fatal Flaw:\nAdagrad has a big problem. It’s a little too cautious. The running tally it keeps of past activity only ever goes up. Over a long training run, the tally for every single knob gets huge. This causes the learning rate for all knobs to shrink and shrink until it’s practically zero. The model completely stops learning. It’s like the manager gets so nervous they refuse to let anyone turn any knobs at all.\nThe Technical Dive Adagrad’s key innovation is to modify the general learning rate $\\eta$ on a per-parameter basis, using a historical record of the gradients.\nThe Update Rule:\nAccumulate squared gradients: At each step, a running cache, $G_{t}$, accumulates the square of the current gradient for each parameter. $$ G_{t, ii} = G_{t-1, ii} + g_{t, i}^{2} $$\nThe update for the $i$-th parameter ($\\theta_{i}$) at time step $t$ is: $$\\theta_{t+1, i} = \\theta_{t, i} - \\frac{\\eta}{\\sqrt{G_{t, ii}} + \\epsilon} \\cdot g_{t, i}$$\n$g_{t, i}$ is the gradient for parameter $i$ at time step $t$.\n$\\eta$ is the global learning rate you set at the beginning.\n$\\epsilon$ (epsilon) is a tiny number (e.g., 1e-8) added for numerical stability to prevent division by zero.\n$G_{t, ii}$, This variable accumulates the sum of the squares of all past gradients for parameter $i$.\nThe summation formula: $G_{t, ii} = \\sum_{k=1}^{t} g_{k, i}^{2}$, it says that at any timestep $t$, the cache $G_{t, ii}$ is the sum of the squares of all gradients from the very first step up to the current one. This is conceptually clear, it’s inefficient to compute. You would have to store every past gradient in memory and re-sum them at every single step.\n$G_{t, ii} = G_{t - 1, ii} + g_{t, i}^{2}$ this is recursive formula, it gives the same result.\nWe know that $G_{t-1}$ already holds the sum of all squared gradients up to the previous step: $G_{t-1, ii} = g_{1, i}^{2} + g_{2, i}^{2} + g_{3, i}^{2} + … + g_{t-1, i}^{2}$.\nto get the new sum $G_{t}$, we simply take that previous sum ($G_{t-1}$) and add the square of the new, current gradient ($g_{t, i}^{2}$).\nWhy It Works:\nIt happens in the denominator: $\\sqrt{G_{t, ii} + \\epsilon}$.\nIf a parameter $i$ has had large gradients in the past, $g_{k, i}^2$ will be large, causing the sum $G_{t, ii}$ to grow very quickly. A large denominator means the effective learning rate ($\\frac{\\eta}{\\sqrt{…}}$) becomes very small for this parameter.\nIf a parameter has had small or zero gradients (like for a rare feature), $G_{t, ii}$ will grow very slowly, keeping the denominator small and the effective learning rate large.\nThe Fatal Flaw Explained Mathematically:\nThe problem is that the sum $G_{t, ii}$ is made of squared (and therefore non-negative) numbers. This sum will monotonically increase throughout training; it will never decrease. As the number of training steps $t$ approaches infinity, $G_{t, ii}$ also approaches infinity. Consequently, the effective learning rate for every parameter will inevitably shrink towards zero, and training will grind to a halt.\nBecause of this, Adagrad is rarely the first choice for training deep neural networks today. However, its core idea of an adaptive, per-parameter learning rate was revolutionary and directly inspired the more advanced optimizers we’ll see next.\nRMSprop We saw that Adagrad had a brilliant idea, custom learning rates for each knob, but a fatal flaw. It’s like a manager who is so traumatized by past mitakes that they eventually become too scared to approve anything. Its memory of past failures builds up forever, paralyzing the entire team. RMSprop is the new managaer who walks in and says, “Let’s focus on what’s happening now.”\nThe Intuitive Idea The Problem: Adagrad’s learning rate only ever goes down. Its agressive accumulation of past squared gradients eventually grinds learning to halt.\nThe Big Idea: RMSprop fixes this with a simple trick: it uses a leaky memory. Instead of a manager who remembers every mistake since the dawn of time, RMSprop is a manager who mostly remembers what happened this week, vaguely remembers last week, and has pretty much forgotten what happened a month ago.\nIt still adapts the learning rate for each knob, but it does so baed on a moving average of recent activity, not the entire history.\nHow it Works:\nAt every step, when RMSprop updates its memory of a knob’s activity, it doesn’t just add the new information to the pile. Instead, it does this:\n“Let’s take 90% of our old memory, and mix 10% of what just happened.”\nThis means the influence of very old events gradually fades away. If a knob was super active a long time ago but isn’t doing much now, RMSprop will slowly forget its chaotic past and be willing to trust it with a larger learning rate again.\nThis prevents the optimizer’s “anxiety” from building up indefinitely. It keeps the good part of Adagrad (the adaptive learning rate) while throwing out the bad part (the paralyzing, ever-growing memory).\nThe result is an optimizer that can continue learning effectively for much, much longer.\nThe Technical Dive RMSprop (Root Mean Square Propagation) modifies Adagrad’s update rule by changing how the gradient accumulation is performed. Instead of a simple sum, it uses an exponentially weighted moving average.\nThe Update Rules:\nThe process involves two steps for each parameter $i$.\nCalculate the moving average of squared gradients: $$ S_{t, i} = \\beta S_{t-1, i} + (1 + \\beta) g_{t, i}^{2} $$\nUpdate the parameters: $$\\theta_{t+1, i} = \\theta_{t, i} - \\frac{\\eta}{\\sqrt{S_{t, i}} + \\epsilon} \\cdot g_{t, i}$$\n$S_{t, i}$ is the new memory component for parameter $i$. It replaces Adagrad’s $G_{t, ii}$. It stands for the moving average of squared gradients.\n$\\beta$ (beta) is the new hyperparameter, known as the decay rate or smoothing constant. It’s typically set to a value like 0.9 or 0.99. It controls the “memory span” of the optimizer.\nThe other variables ($\\theta$, $\\eta$, $g_{t, i}$, $\\epsilon$) are the same as before.\nWhy It Works:\nThe key is the first equation. Let’s compare it to Adagrad’s accumulation:\nAdagrad: $G_{t, ii} = G_{t-1, ii} + g_{t, i}^2$ (a simple, ever-growing sum) RMSprop: $S_{t, i} = \\beta S_{t-1, i} + (1 - \\beta) g_{t, i}^2$ (a weighted average) The presence of the decay rate $\\beta$ is the entire fix. Because we’re always multiplying the old memory $S_{t-1, i}$ by a number less than 1 (e.g., 0.9), the influence of old gradients exponentially decays. They are slowly “forgotten.”\nThis prevents the denominator ($\\sqrt{S_{t, i} + \\epsilon}$) from growing infinitely large. It keeps the effective learning rate adaptive but prevents it from vanishing, allowing learning to proceed.\nKey Hyperparameters: Learning Rate ($\\eta$): A good starting point is often 0.001 Decay Rate ($\\beta$): The default of 0.9 or 0.99 works well in most cases. You rarely need to tune this. ",
  "wordCount" : "1488",
  "inLanguage": "en",
  "datePublished": "2025-09-22T00:00:00Z",
  "dateModified": "2025-09-22T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Lav-niya"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://lav-aniya.github.io/blogs/optimizers_2/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Lav-Aniya's Site",
    "logo": {
      "@type": "ImageObject",
      "url": "https://lav-aniya.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://lav-aniya.github.io/" accesskey="h" title="Lav-Aniya&#39;s Site (Alt + H)">Lav-Aniya&#39;s Site</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://github.com/Lav-aniya" title="&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-width=&#34;2&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34;&gt;&lt;path d=&#34;M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22&#34;&gt;&lt;/path&gt;&lt;/svg&gt;">
                    <span><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"></path></svg></span>
                    
                </a>
            </li>
            <li>
                <a href="https://www.linkedin.com/in/your-profile-name/" title="&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-width=&#34;2&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34;&gt;&lt;path d=&#34;M16 8a6 6 0 0 1 6 6v7h-4v-7a2 2 0 0 0-2-2 2 2 0 0 0-2 2v7h-4v-7a6 6 0 0 1 6-6z&#34;&gt;&lt;/path&gt;&lt;rect x=&#34;2&#34; y=&#34;9&#34; width=&#34;4&#34; height=&#34;12&#34;&gt;&lt;/rect&gt;&lt;circle cx=&#34;4&#34; cy=&#34;4&#34; r=&#34;2&#34;&gt;&lt;/circle&gt;&lt;/svg&gt;">
                    <span><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M16 8a6 6 0 0 1 6 6v7h-4v-7a2 2 0 0 0-2-2 2 2 0 0 0-2 2v7h-4v-7a6 6 0 0 1 6-6z"></path><rect x="2" y="9" width="4" height="12"></rect><circle cx="4" cy="4" r="2"></circle></svg></span>
                    
                </a>
            </li>
            <li>
                <a href="mailto:lavaniya.nen@gmail.com" title="&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-width=&#34;2&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34;&gt;&lt;path d=&#34;M4 4h16c1.1 0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1 0-2-.9-2-2V6c0-1.1.9-2 2-2z&#34;&gt;&lt;/path&gt;&lt;polyline points=&#34;22,6 12,13 2,6&#34;&gt;&lt;/polyline&gt;&lt;/svg&gt;">
                    <span><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M4 4h16c1.1 0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1 0-2-.9-2-2V6c0-1.1.9-2 2-2z"></path><polyline points="22,6 12,13 2,6"></polyline></svg></span>
                    
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      O2
    </h1>
    <div class="post-description">
      An Introduction to Optimization
    </div>
    <div class="post-meta"><span title='2025-09-22 00:00:00 +0000 UTC'>September 22, 2025</span>&nbsp;·&nbsp;7 min&nbsp;·&nbsp;Lav-niya

</div>
  </header> 
  <div class="post-content"><p>This is the second part of the series. You can read the first part here: <a href="/blogs/optimizers/">O1</a></p>
<p>We have seen that SGD is clumsy and Momentum is a bit smoother but still uses a one-size-fits-all approach. Now, let&rsquo;s meet an optimizer that tries to be a little smarter.</p>
<hr>
<h2 id="adagrad-the-adaptive-gradient-optimizer">Adagrad: The Adaptive Gradient Optimizer<a hidden class="anchor" aria-hidden="true" href="#adagrad-the-adaptive-gradient-optimizer">#</a></h2>
<p>SGD and Momentum, they are like mechanics who uses the exact same wrench with the exact force on every single bold of an engine. This is a problem. Some bolts are tiny and sensitive, while others are big and rusty. You can&rsquo;t treat them all the same. Adagrad is the first applicant who walks in with a full, custom toolkit.</p>
<h3 id="the-intuitive-idea">The Intuitive Idea<a hidden class="anchor" aria-hidden="true" href="#the-intuitive-idea">#</a></h3>
<p><strong>The Problem:</strong> Your model can have millions of parameters (knobs). Some of these knobs might be very important and get updated frequently. Others might control very niche features and are rarely touched.
Using the same learning rate (step size) for all of them is inefficient.</p>
<p><strong>The Big Idea:</strong> Adagrad gives a unique, <strong>custom learning rate to every single knob.</strong> It <em>adapts</em> the learning rate based on how much that specific knob has turned in the past. This is why its name is <strong>Ada</strong>grad, which stands for Adaptive Gradient.</p>
<p><strong>How it Works:</strong></p>
<p>Adagrad is like a cautious manager. It keeps a running tally of how &ldquo;active&rdquo; each knob has been.</p>
<ul>
<li>For knobs that have seen a lot of action (frequent, large updates), Adagrad gets nervous. It thinks, &ldquo;Whoa, this knob is really sensitive. Let&rsquo;s be gentle with it.&rdquo; It <strong>dramatically shrinks the learning rate</strong> for that specific knob.</li>
<li>For knobs that have barely been touched (infrequent updates, like for rare words in a vocabulary), Adagrad thinks, &ldquo;This one&rsquo;s been ignored. Let&rsquo;s give it a bigger push to see what happens.&rdquo; It <strong>keeps a larger learning rate</strong> for that knob.</li>
</ul>
<p>This is a game-changer for problems with sparse data. For example, in language models, the parameter for a common word like &ldquo;the&rdquo; will get small updates, while the parameter for a rare word like &ldquo;aficionado&rdquo; will get much larger updates, helping it learn effectively even though it appears less often.</p>
<p><strong>The Fatal Flaw:</strong></p>
<p>Adagrad has a big problem. It&rsquo;s a little too cautious. The running tally it keeps of past activity <strong>only ever goes up.</strong> Over a long training run, the tally for every single knob gets huge. This causes the learning rate for all knobs to shrink and shrink until it&rsquo;s practically zero. The model completely stops learning. It&rsquo;s like the manager gets so nervous they refuse to let anyone turn any knobs at all.</p>
<p><img alt="Explained the fatal flaw" loading="lazy" src="/optimizers/adagrad_fatal_flaw.png"></p>
<h3 id="the-technical-dive">The Technical Dive<a hidden class="anchor" aria-hidden="true" href="#the-technical-dive">#</a></h3>
<p>Adagrad&rsquo;s key innovation is to modify the general learning rate $\eta$ on a per-parameter basis, using a historical record of the gradients.</p>
<p><strong>The Update Rule:</strong></p>
<ol>
<li>Accumulate squared gradients: At each step, a running cache, $G_{t}$,  accumulates the square of the current gradient for each parameter.</li>
</ol>
<p>$$ G_{t, ii} = G_{t-1, ii} + g_{t, i}^{2} $$</p>
<ol start="2">
<li>The update for the $i$-th parameter ($\theta_{i}$) at time step $t$ is:</li>
</ol>
<p>$$\theta_{t+1, i} = \theta_{t, i} - \frac{\eta}{\sqrt{G_{t, ii}} + \epsilon} \cdot g_{t, i}$$</p>
<ul>
<li>
<p>$g_{t, i}$ is the gradient for parameter $i$ at time step $t$.</p>
</li>
<li>
<p>$\eta$ is the global learning rate you set at the beginning.</p>
</li>
<li>
<p>$\epsilon$ (epsilon) is a tiny number (e.g., <code>1e-8</code>) added for numerical stability to prevent division by zero.</p>
</li>
<li>
<p>$G_{t, ii}$, This variable accumulates the sum of the squares of all past gradients for parameter $i$.</p>
<ul>
<li>
<p>The summation formula: $G_{t, ii} = \sum_{k=1}^{t} g_{k, i}^{2}$, it says that at any timestep $t$, the cache $G_{t, ii}$ is the sum of the squares of all gradients from the very first step up to the current one. This is conceptually clear, it&rsquo;s inefficient to compute. You would have to store every past gradient in memory and re-sum them at every single step.</p>
</li>
<li>
<p>$G_{t, ii} = G_{t - 1, ii} + g_{t, i}^{2}$ this is recursive formula, it gives the same result.</p>
<ul>
<li>
<p>We know that $G_{t-1}$ already holds the sum of all squared gradients up to the previous step: $G_{t-1, ii} = g_{1, i}^{2} + g_{2, i}^{2} + g_{3, i}^{2} + &hellip; + g_{t-1, i}^{2}$.</p>
</li>
<li>
<p>to get the new sum $G_{t}$, we simply take that previous sum ($G_{t-1}$) and add the square of the new, current gradient ($g_{t, i}^{2}$).</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><strong>Why It Works:</strong></p>
<p>It happens in the denominator: $\sqrt{G_{t, ii} + \epsilon}$.</p>
<ul>
<li>
<p>If a parameter $i$ has had large gradients in the past, $g_{k, i}^2$ will be large, causing the sum $G_{t, ii}$ to grow very quickly. A large denominator means the effective learning rate ($\frac{\eta}{\sqrt{&hellip;}}$) becomes very small for this parameter.</p>
</li>
<li>
<p>If a parameter has had small or zero gradients (like for a rare feature), $G_{t, ii}$ will grow very slowly, keeping the denominator small and the effective learning rate large.</p>
</li>
</ul>
<p><strong>The Fatal Flaw Explained Mathematically:</strong></p>
<p>The problem is that the sum $G_{t, ii}$ is made of squared (and therefore non-negative) numbers. This sum will monotonically increase throughout training; it will never decrease. As the number of training steps $t$ approaches infinity, $G_{t, ii}$ also approaches infinity. Consequently, the effective learning rate for every parameter will inevitably shrink towards zero, and training will grind to a halt.</p>
<p>Because of this, Adagrad is rarely the first choice for training deep neural networks today. However, its core idea of an adaptive, per-parameter learning rate was revolutionary and directly inspired the more advanced optimizers we&rsquo;ll see next.</p>
<hr>
<h2 id="rmsprop">RMSprop<a hidden class="anchor" aria-hidden="true" href="#rmsprop">#</a></h2>
<p>We saw that Adagrad had a brilliant idea, custom learning rates for each knob, but a fatal flaw. It&rsquo;s like a manager who is so traumatized by past mitakes that they eventually become too scared to approve anything. Its memory of past failures builds up forever, paralyzing the entire team. RMSprop is the new managaer who walks in and says, &ldquo;Let&rsquo;s focus on what&rsquo;s happening <em>now</em>.&rdquo;</p>
<h3 id="the-intuitive-idea-1">The Intuitive Idea<a hidden class="anchor" aria-hidden="true" href="#the-intuitive-idea-1">#</a></h3>
<p><strong>The Problem:</strong> Adagrad&rsquo;s learning rate only ever goes down. Its agressive accumulation of past squared gradients eventually grinds learning to halt.</p>
<p><strong>The Big Idea:</strong> RMSprop fixes this with a simple trick: it uses a <strong>leaky memory</strong>. Instead of a manager who remembers every mistake since the dawn of time, RMSprop is a manager who mostly remembers what happened this week, vaguely remembers last week, and has pretty much forgotten what happened a month ago.</p>
<p>It still adapts the learning rate for each knob, but it does so baed on a <strong>moving average</strong> of recent activity, not the entire history.</p>
<p><strong>How it Works:</strong></p>
<p>At every step, when RMSprop updates its memory of a knob&rsquo;s activity, it doesn&rsquo;t just add the new information to the pile. Instead, it does this:</p>
<p>&ldquo;Let&rsquo;s take 90% of our old memory, and mix 10% of what <em>just</em> happened.&rdquo;</p>
<p>This means the influence of very old events gradually fades away. If a knob was super active a long time ago but isn&rsquo;t doing much now, RMSprop will slowly forget its chaotic past and be willing to trust it with a larger learning rate again.</p>
<p>This prevents the optimizer&rsquo;s &ldquo;anxiety&rdquo; from building up indefinitely. It keeps the good part of Adagrad (the adaptive learning rate) while throwing out the bad part (the paralyzing, ever-growing memory).</p>
<p><img alt="A gif explaining Adagrad vs RMSprop" loading="lazy" src="/optimizers/gifs/transparent_AdagradvsRMSprop.gif"></p>
<p>The result is an optimizer that can continue learning effectively for much, much longer.</p>
<h3 id="the-technical-dive-1">The Technical Dive<a hidden class="anchor" aria-hidden="true" href="#the-technical-dive-1">#</a></h3>
<p>RMSprop (Root Mean Square Propagation) modifies Adagrad&rsquo;s update rule by changing how the gradient accumulation is performed. Instead of a simple sum, it uses an exponentially weighted moving average.</p>
<p><strong>The Update Rules:</strong></p>
<p>The process involves two steps for each parameter $i$.</p>
<ol>
<li><strong>Calculate the moving average of squared gradients:</strong></li>
</ol>
<p>$$ S_{t, i} = \beta S_{t-1, i} + (1 + \beta) g_{t, i}^{2} $$</p>
<ol start="2">
<li><strong>Update the parameters:</strong></li>
</ol>
<p>$$\theta_{t+1, i} = \theta_{t, i} - \frac{\eta}{\sqrt{S_{t, i}} + \epsilon} \cdot g_{t, i}$$</p>
<ul>
<li>
<p>$S_{t, i}$ is the new memory component for parameter $i$. It replaces      Adagrad&rsquo;s $G_{t, ii}$. It stands for the <strong>moving average of squared gradients</strong>.</p>
</li>
<li>
<p>$\beta$ (beta) is the new hyperparameter, known as the <strong>decay rate</strong> or <strong>smoothing constant</strong>. It&rsquo;s typically set to a value like <code>0.9</code> or <code>0.99</code>. It controls the &ldquo;memory span&rdquo; of the optimizer.</p>
</li>
<li>
<p>The other variables ($\theta$, $\eta$, $g_{t, i}$, $\epsilon$) are the same as before.</p>
</li>
</ul>
<p><strong>Why It Works:</strong></p>
<p>The key is the first equation. Let&rsquo;s compare it to Adagrad&rsquo;s accumulation:</p>
<ul>
<li>Adagrad: $G_{t, ii} = G_{t-1, ii} + g_{t, i}^2$ (a simple, ever-growing sum)</li>
<li>RMSprop: $S_{t, i} = \beta S_{t-1, i} + (1 - \beta) g_{t, i}^2$ (a weighted average)</li>
</ul>
<p>The presence of the decay rate $\beta$ is the entire fix. Because we&rsquo;re always multiplying the old memory $S_{t-1, i}$ by a number less than 1 (e.g., 0.9), the influence of old gradients exponentially decays. They are slowly &ldquo;forgotten.&rdquo;</p>
<p><img alt="memory bank" loading="lazy" src="/optimizers/memory_banks_adagrad_rmsprop.png"></p>
<p>This prevents the denominator ($\sqrt{S_{t, i} + \epsilon}$) from growing infinitely large. It keeps the effective learning rate adaptive but prevents it from vanishing, allowing learning to proceed.</p>
<h3 id="key-hyperparameters">Key Hyperparameters:<a hidden class="anchor" aria-hidden="true" href="#key-hyperparameters">#</a></h3>
<ol>
<li><strong>Learning Rate ($\eta$):</strong> A good starting point is often <code>0.001</code></li>
<li><strong>Decay Rate ($\beta$):</strong> The default of <code>0.9</code> or <code>0.99</code> works well in most cases. You rarely need to tune this.</li>
</ol>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://lav-aniya.github.io/tags/optimization/">Optimization</a></li>
      <li><a href="https://lav-aniya.github.io/tags/deep-learning/">Deep Learning</a></li>
    </ul>
<nav class="paginav">
  <a class="next" href="https://lav-aniya.github.io/blogs/optimizers/">
    <span class="title">Next »</span>
    <br>
    <span>O1</span>
  </a>
</nav>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share O2 on x"
            href="https://x.com/intent/tweet/?text=O2&amp;url=https%3a%2f%2flav-aniya.github.io%2fblogs%2foptimizers_2%2f&amp;hashtags=Optimization%2cDeepLearning">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share O2 on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2flav-aniya.github.io%2fblogs%2foptimizers_2%2f&amp;title=O2&amp;summary=O2&amp;source=https%3a%2f%2flav-aniya.github.io%2fblogs%2foptimizers_2%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share O2 on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2flav-aniya.github.io%2fblogs%2foptimizers_2%2f&title=O2">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share O2 on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2flav-aniya.github.io%2fblogs%2foptimizers_2%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share O2 on whatsapp"
            href="https://api.whatsapp.com/send?text=O2%20-%20https%3a%2f%2flav-aniya.github.io%2fblogs%2foptimizers_2%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share O2 on telegram"
            href="https://telegram.me/share/url?text=O2&amp;url=https%3a%2f%2flav-aniya.github.io%2fblogs%2foptimizers_2%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share O2 on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=O2&u=https%3a%2f%2flav-aniya.github.io%2fblogs%2foptimizers_2%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://lav-aniya.github.io/">Lav-Aniya&#39;s Site</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
